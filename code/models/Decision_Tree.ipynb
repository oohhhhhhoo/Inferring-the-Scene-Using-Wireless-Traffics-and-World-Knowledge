{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "EIo9gb1AeYF6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "fOVyQrB4fDbB"
   },
   "outputs": [],
   "source": [
    "def slide_split_get_label_from_filename(filename):\n",
    "    # print(label_mapping.get(filename[9:-5],-1))\n",
    "    return label_mapping.get(filename[9:-5],-1)\n",
    "\n",
    "def split_get_label_from_filename(filename):\n",
    "    return label_mapping.get(filename[8:-5],-1)\n",
    "\n",
    "def get_label_from_filename(filename):\n",
    "    match = re.search(r'_(\\d+_?\\d*)\\.json$', filename)\n",
    "    print(match)\n",
    "    if match:\n",
    "        label_key = match.group(1)\n",
    "        return label_mapping.get(label_key, -1)  # 如果标签不匹配返回 -1\n",
    "\n",
    "# 获取文件夹中的文件及其标签\n",
    "def load_files_and_labels(folder_path,split=False):\n",
    "    file_list = []\n",
    "    labels = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        # print(\"file: \", file)\n",
    "        if file.endswith(\".json\"):\n",
    "            if split == 1 or split==2 or split==3:\n",
    "                label = split_get_label_from_filename(file)\n",
    "            elif split == 4: \n",
    "                # print(f\"split is {split}\")\n",
    "                label = slide_split_get_label_from_filename(file)\n",
    "            else: \n",
    "                label = get_label_from_filename(file)\n",
    "            if label != -1:\n",
    "                file_list.append(os.path.join(folder_path, file))\n",
    "                labels.append(label)\n",
    "            #print(f\"File: {file}, Label: {label}\")\n",
    "\n",
    "    return file_list, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "4mfG863jfGeV"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# 1. 提取字符并编码为整数索引\n",
    "def hex_to_sequence(hex_feature):\n",
    "    \"\"\"\n",
    "    将十六进制字符串转换为整数索引序列。\n",
    "    去掉冒号并转换为字符对应的索引。\n",
    "    \"\"\"\n",
    "    hex_chars = hex_feature.replace(\":\", \"\")\n",
    "    char_to_index = {char: idx for idx, char in enumerate(\"0123456789abcdef\")}\n",
    "    return [char_to_index[char] for char in hex_chars]\n",
    "\n",
    "def process_json(file_path):\n",
    "    \"\"\"\n",
    "    处理 JSON 文件，提取时间戳、包长度和 raw data 特征，并对特征进行归一化。\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    try:\n",
    "        # 打开 JSON 文件\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            datas = json.load(file)\n",
    "\n",
    "        # 遍历 JSON 数据，提取时间戳和包长度\n",
    "        initial_timestamp = None\n",
    "        pre_time=None\n",
    "        for data in datas:\n",
    "            try:\n",
    "                timestamp = float(data[\"_source\"][\"layers\"][\"frame\"][\"frame.time_relative\"])  # 时间戳\n",
    "                packet_length = int(data[\"_source\"][\"layers\"][\"frame\"][\"frame.len\"])  # 包长度\n",
    "\n",
    "                # # 获取数据部分，若不存在则为0\n",
    "                # if 'data' in data[\"_source\"][\"layers\"]:\n",
    "                #     rawdata = data[\"_source\"][\"layers\"][\"data\"][\"data.data\"]\n",
    "                # else:\n",
    "                #     rawdata = '0'\n",
    "\n",
    "                # 将原始数据转换为整数序列\n",
    "                # data_feature = hex_to_sequence(rawdata)\n",
    "\n",
    "\n",
    "                # # 将数据填充或截断为指定长度 2832\n",
    "                # if len(data_feature) < 2832:  # 如果长度小于 2832，填充 0\n",
    "                #     data_feature += [0] * (2832 - len(data_feature))\n",
    "                # elif len(data_feature) > 2832:  # 如果长度大于 2832，截断\n",
    "                #     data_feature = data_feature[:2832]\n",
    "\n",
    "                # 如果数据长度不符合预期，打印出来\n",
    "                # if len(data_feature) != 2832:\n",
    "                #     print(f\"Data feature length mismatch: {len(data_feature)}\")\n",
    "\n",
    "                # 初始化时间戳\n",
    "                if initial_timestamp is None:\n",
    "                    initial_timestamp = timestamp  # 设置初始时间戳\n",
    "\n",
    "\n",
    "                # 计算相对时间戳\n",
    "\n",
    "                relative_timestamp = timestamp - initial_timestamp\n",
    "                if pre_time==None:\n",
    "                    pre_time=relative_timestamp\n",
    "\n",
    "                time_diff=relative_timestamp-pre_time\n",
    "                pre_time=relative_timestamp\n",
    "\n",
    "                # print(type(relative_timestamp))\n",
    "\n",
    "\n",
    "                timestamp_array = np.array([relative_timestamp], dtype=float)\n",
    "                time_diff_array = np.array([time_diff], dtype=float)\n",
    "\n",
    "                packet_length_array = np.array([packet_length], dtype=float)/1512\n",
    "                # data_feature = np.array(data_feature, dtype=float)/15\n",
    "\n",
    "                # 将特征按顺序组合为 [时间戳, 包长度, 数据特征]\n",
    "                # feature = np.hstack((timestamp_array,time_diff_array, packet_length_array,data_feature))\n",
    "\n",
    "                feature = np.hstack((timestamp_array,time_diff_array, packet_length_array))\n",
    "\n",
    "                # 添加到特征列表\n",
    "                features.append(feature)\n",
    "\n",
    "            except (KeyError, ValueError) as e:\n",
    "                # 跳过有问题的数据包\n",
    "                print(f\"Skipping packet due to error: {e}\")\n",
    "                continue\n",
    "\n",
    "        features_array=np.array(features)\n",
    "        max_timestamp = np.max(features_array[:, 0])  # 获取最大时间戳\n",
    "        max_time_diff = np.max(features_array[:, 1])  # 获取最大时间diff\n",
    "        # print(\"max\")\n",
    "        features_array[:, 0] = [feature[0] / max_timestamp for feature in features_array]  # 时间戳归一化\n",
    "        features_array[:, 1] = [feature[1] / max_time_diff for feature in features_array]  # 时间diff归一化\n",
    "        # print(\"11\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(Exception)\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        features_array = np.zeros((1, 3))  # 返回空特征以避免程序中断\n",
    "\n",
    "    return features_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "-OpQ-4LAfKVs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "<class 'list'>\n",
      "(800,)\n",
      "[1 0 2 2 1 3 1 3 1 1 2 1 0 0 3 0 0 0 2 0 2 3 2 3 0 3 0 3 3 0 0 1 2 1 3 1 2\n",
      " 0 2 1 3 3 0 0 2 2 2 3 0 3 3 1 2 1 1 3 0 0 3 3 2 0 1 1 1 0 0 3 0 1 3 2 3 3\n",
      " 1 1 0 2 0 2 3 1 3 0 1 0 0 3 1 3 3 0 1 3 1 1 2 0 3 3 1 1 0 0 1 0 3 3 2 0 2\n",
      " 2 2 3 0 3 2 3 0 3 1 0 0 3 2 0 0 0 0 2 0 3 0 2 3 3 1 0 3 0 1 2 1 2 0 2 0 3\n",
      " 0 1 1 0 2 1 3 0 0 0 3 2 1 1 2 3 3 0 1 3 1 3 3 3 2 1 1 2 3 2 1 1 1 2 2 0 0\n",
      " 0 1 2 2 2 2 0 1 2 1 1 1 2 3 0 1 0 2 3 2 2 0 2 1 1 3 0 3 2 2 1 2 3 0 2 3 0\n",
      " 1 1 0 3 2 1 3 1 2 3 2 1 3 1 0 0 1 3 1 1 2 0 3 3 1 3 3 0 3 3 3 0 2 2 1 1 0\n",
      " 3 2 2 3 3 2 1 1 2 2 1 3 3 3 0 0 3 0 2 3 0 2 0 2 0 2 2 0 0 0 2 2 1 2 0 0 2\n",
      " 3 3 3 1 0 2 2 0 2 1 3 0 0 1 3 0 3 1 0 0 1 0 2 2 2 2 3 2 3 3 2 1 2 1 3 3 1\n",
      " 1 3 1 1 2 1 1 1 3 0 3 0 3 0 0 2 2 0 3 1 1 0 1 1 0 3 0 0 3 3 0 1 0 1 0 3 3\n",
      " 0 0 1 1 0 0 2 0 3 1 3 3 3 0 0 2 2 2 2 1 2 2 0 1 0 2 1 1 3 0 2 2 2 2 3 1 1\n",
      " 1 2 3 1 1 2 1 2 3 0 3 3 3 1 3 2 2 1 1 1 2 2 1 2 2 0 0 0 3 2 3 0 0 2 1 3 3\n",
      " 3 2 3 0 3 3 1 2 2 0 0 0 0 0 3 2 1 0 0 0 2 0 3 2 1 0 1 3 2 2 2 3 3 0 1 3 2\n",
      " 3 0 2 1 0 3 3 3 1 1 3 2 3 3 0 0 3 2 1 0 2 1 1 0 1 3 3 2 1 2 1 1 2 2 2 2 2\n",
      " 1 2 3 2 2 1 1 3 1 2 1 0 0 3 3 0 1 2 0 0 3 2 0 2 2 0 3 1 2 1 1 3 1 3 3 0 1\n",
      " 2 2 1 1 1 2 3 2 0 1 2 2 3 2 2 3 1 0 1 1 2 1 3 1 2 1 2 0 0 3 3 1 0 0 3 3 0\n",
      " 3 3 3 2 2 1 2 2 1 1 2 1 1 1 0 3 2 3 3 2 0 0 2 0 3 3 1 3 1 1 2 1 1 3 2 3 1\n",
      " 1 1 0 3 0 3 0 2 1 2 1 3 2 3 2 1 0 2 3 2 1 0 3 1 3 0 1 2 0 2 2 0 3 0 1 1 2\n",
      " 2 2 0 2 0 1 2 1 1 0 3 2 3 1 0 2 0 2 2 0 0 2 2 1 2 2 0 3 2 0 1 1 2 2 1 3 0\n",
      " 3 1 3 3 0 0 1 3 3 0 0 0 1 2 1 0 3 1 0 0 1 1 1 1 1 0 2 3 3 2 2 3 0 0 1 1 3\n",
      " 1 1 0 2 2 0 1 3 2 3 3 3 2 3 0 0 3 2 1 3 1 0 3 1 1 2 0 0 1 0 2 0 1 3 2 0 0\n",
      " 2 3 3 3 0 2 3 2 3 1 2 3 0 1 1 2 0 3 1 3 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 定义文件夹路径 /content/drive/MyDrive/202_project/202_packet_json\n",
    "\n",
    "split = 2 # 0:100, 1:300, 2:800 3:1000 4:3500\n",
    "# folder_path = \"packet_json_split\"\n",
    "if split==1:\n",
    "    folder_path = \"packet_json_split\"\n",
    "elif split==2:\n",
    "    folder_path = \"202_packet_json_new_800\"\n",
    "elif split==3:\n",
    "    folder_path = \"202_packet_json_new_1000\"\n",
    "elif split==4:\n",
    "    folder_path = \"202_packet_json_new_3500\"\n",
    "else:\n",
    "    folder_path = \"202_packet_json_new\"\n",
    "\n",
    "# 定义标签映射\n",
    "label_mapping = {\n",
    "    \"0\": 0,             # static\n",
    "    \"0_2\": 1,           # slightly_move\n",
    "    \"1\": 2,             # move\n",
    "    \"4\": 3              # intensely_move\n",
    "}\n",
    "\n",
    "# 调用函数获取文件和标签\n",
    "data_paths, labels = load_files_and_labels(folder_path,split)\n",
    "# 用于存储每个样本的特征\n",
    "\n",
    "print(type(labels))\n",
    "labels_array=np.array(labels)#[:50]\n",
    "print(labels_array.shape)\n",
    "print(labels_array)\n",
    "labels_tensor=torch.tensor(labels_array,dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "KxgivuFtfNoL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成了 800 个样本特征\n",
      "第一个样本的特征形状: (1486, 3)\n"
     ]
    }
   ],
   "source": [
    "features_list = []\n",
    "for data_path in data_paths:\n",
    "  features = process_json(data_path)\n",
    "  features_list.append(features)\n",
    "print(f\"生成了 {len(features_list)} 个样本特征\")\n",
    "print(f\"第一个样本的特征形状: {features_list[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "3WCsh2anaDHn"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. DecisionTreeClassifier expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 76\u001b[0m\n\u001b[1;32m     73\u001b[0m decision_tree \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier(criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgini\u001b[39m\u001b[38;5;124m'\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m \u001b[43mdecision_tree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# 在验证集上评估\u001b[39;00m\n\u001b[1;32m     79\u001b[0m eval_predictions \u001b[38;5;241m=\u001b[39m decision_tree\u001b[38;5;241m.\u001b[39mpredict(eval_features)\n",
      "File \u001b[0;32m~/anaconda3/envs/202_proj/lib/python3.9/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/202_proj/lib/python3.9/site-packages/sklearn/tree/_classes.py:1009\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    980\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \n\u001b[1;32m    982\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1009\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/202_proj/lib/python3.9/site-packages/sklearn/tree/_classes.py:252\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    248\u001b[0m check_X_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    249\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mDTYPE, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    250\u001b[0m )\n\u001b[1;32m    251\u001b[0m check_y_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 252\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_separately\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcheck_X_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_y_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m missing_values_in_feature_mask \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_missing_values_in_feature_mask(X)\n\u001b[1;32m    258\u001b[0m )\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n",
      "File \u001b[0;32m~/anaconda3/envs/202_proj/lib/python3.9/site-packages/sklearn/base.py:645\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m check_X_params:\n\u001b[1;32m    644\u001b[0m     check_X_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdefault_check_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_X_params}\n\u001b[0;32m--> 645\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_X_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m check_y_params:\n\u001b[1;32m    647\u001b[0m     check_y_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdefault_check_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params}\n",
      "File \u001b[0;32m~/anaconda3/envs/202_proj/lib/python3.9/site-packages/sklearn/utils/validation.py:1058\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1054\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1055\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1056\u001b[0m     )\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nd \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m-> 1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1061\u001b[0m     )\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m   1064\u001b[0m     _assert_all_finite(\n\u001b[1;32m   1065\u001b[0m         array,\n\u001b[1;32m   1066\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m   1067\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m   1068\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1069\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. DecisionTreeClassifier expected <= 2."
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# 加载特征和标签，代码与之前一致\n",
    "features_list = []\n",
    "for data_path in data_paths:\n",
    "    features = process_json(data_path)\n",
    "    features_list.append(features)\n",
    "\n",
    "# Flatten features and ensure所有特征矩阵具有相同形状\n",
    "if split==1:fixed_time_steps = 3000\n",
    "elif split==2: fixed_time_steps = 1600\n",
    "elif split==3: fixed_time_steps = 1200\n",
    "elif split==4: fixed_time_steps = 1600\n",
    "else: fixed_time_steps = 12000\n",
    "\n",
    "\n",
    "# target_dim = 256  # 降到 256 维\n",
    "# pca = PCA(n_components=target_dim)\n",
    "\n",
    "# 降维处理\n",
    "# reduced_features_list = []\n",
    "# for feature in features_list:\n",
    "#     reduced_feature = pca.fit_transform(feature)\n",
    "#     reduced_features_list.append(reduced_feature)\n",
    "\n",
    "# 截断或补零到固定时间步长\n",
    "aligned_features = []\n",
    "for feature in features_list:\n",
    "    if feature.shape[0] > fixed_time_steps:\n",
    "        truncated = feature[:fixed_time_steps, :]\n",
    "    else:\n",
    "        truncated = np.pad(feature, ((0, fixed_time_steps - feature.shape[0]), (0, 0)), mode='constant')\n",
    "    aligned_features.append(truncated)\n",
    "\n",
    "# 展平特征\n",
    "# aligned_features_flat = [feature.flatten() for feature in aligned_features]\n",
    "# features_array = np.array(aligned_features_flat)\n",
    "\n",
    "features_array = np.array(aligned_features)\n",
    "# 转换标签为 NumPy 数组\n",
    "labels_array = np.array(labels)\n",
    "\n",
    "# Split the dataset into train and validation sets\n",
    "if split == 1:\n",
    "    train_features = features_array[:210]\n",
    "    train_labels = labels_array[:210]\n",
    "    eval_features = features_array[210:]\n",
    "    eval_labels = labels_array[210:]\n",
    "elif split == 2:\n",
    "    train_features = features_array[:600]\n",
    "    train_labels = labels_array[:600]\n",
    "    eval_features = features_array[600:]\n",
    "    eval_labels = labels_array[600:]\n",
    "elif split == 3:\n",
    "    train_features = features_array[:800]\n",
    "    train_labels = labels_array[:800]\n",
    "    eval_features = features_array[800:]\n",
    "    eval_labels = labels_array[800:]\n",
    "elif split ==4:\n",
    "    train_features = features_array[:3200]\n",
    "    train_labels = labels_array[:3200]\n",
    "    eval_features = features_array[3200:]\n",
    "    eval_labels = labels_array[3200:]    \n",
    "else:\n",
    "    train_features = features_array[:70]\n",
    "    train_labels = labels_array[:70]\n",
    "    eval_features = features_array[70:]\n",
    "    eval_labels = labels_array[70:]\n",
    "# 初始化决策树模型\n",
    "# criterion 可选 'gini'（默认）或 'entropy'，max_depth 控制树的深度\n",
    "decision_tree = DecisionTreeClassifier(criterion='gini', max_depth=10, random_state=42)\n",
    "\n",
    "# 训练模型\n",
    "decision_tree.fit(train_features, train_labels)\n",
    "\n",
    "# 在验证集上评估\n",
    "eval_predictions = decision_tree.predict(eval_features)\n",
    "eval_accuracy = accuracy_score(eval_labels, eval_predictions)\n",
    "\n",
    "print(f\"Validation Accuracy: {eval_accuracy * 100:.2f}%\")\n",
    "\n",
    "# 在训练集上评估（可选）\n",
    "train_predictions = decision_tree.predict(train_features)\n",
    "train_accuracy = accuracy_score(train_labels, train_predictions)\n",
    "print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "\n",
    "# 打印分类报告\n",
    "print(\"\\nClassification Report (Validation Set):\")\n",
    "print(classification_report(eval_labels, eval_predictions))\n",
    "\n",
    "# # 可视化决策树（需要 Graphviz）\n",
    "# from sklearn.tree import export_graphviz\n",
    "# import graphviz\n",
    "\n",
    "# # 将决策树导出为 .dot 格式\n",
    "# dot_data = export_graphviz(\n",
    "#     decision_tree, out_file=None,\n",
    "#     feature_names=[f\"feature_{i}\" for i in range(features_array.shape[1])],\n",
    "#     class_names=[str(cls) for cls in np.unique(labels_array)],\n",
    "#     filled=True, rounded=True,\n",
    "#     special_characters=True\n",
    "# )\n",
    "\n",
    "# # 使用 graphviz 渲染决策树\n",
    "# graph = graphviz.Source(dot_data)\n",
    "# graph.render(\"decision_tree\")  # 保存为文件 decision_tree.pdf\n",
    "# graph.view()  # 打开生成的决策树图像\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "202_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
