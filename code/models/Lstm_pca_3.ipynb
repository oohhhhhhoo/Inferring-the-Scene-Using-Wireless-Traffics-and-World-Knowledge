{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "YGw4jevKM2We"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "Py5ePQ_giXN1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 定义设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 标签映射\n",
    "label_mapping = {\n",
    "    \"0\": 0,             # static\n",
    "    \"0_2\": 1,           # slightly_move\n",
    "    \"1\": 2,             # move\n",
    "    \"4\": 3              # intensely_move\n",
    "}\n",
    "\n",
    "# split 参数控制数据路径和特征参数\n",
    "split = 1  # 0:100, 1:300, 2:800, 3:1000, 4:3500\n",
    "split_configs = {\n",
    "    1: {\"train_folder\": \"202_packet_json_new_300_train\", \"eval_folder\": \"202_packet_json_new_300_eval\", \"fixed_time_steps\": 3000},\n",
    "    2: {\"train_folder\": \"202_packet_json_new_800_train\", \"eval_folder\": \"202_packet_json_new_800_eval\", \"fixed_time_steps\": 1600},\n",
    "    3: {\"train_folder\": \"202_packet_json_new_1000_train\", \"eval_folder\": \"202_packet_json_new_1000_eval\", \"fixed_time_steps\": 1200},\n",
    "    4: {\"train_folder\": \"202_packet_json_new_3600_train\", \"eval_folder\": \"202_packet_json_new_3600_eval\", \"fixed_time_steps\": 1600},\n",
    "    0: {\"train_folder\": \"202_packet_json_new_train\", \"eval_folder\": \"202_packet_json_new_eval\", \"fixed_time_steps\": 12000}\n",
    "}\n",
    "config = split_configs[split]\n",
    "train_folder = config[\"train_folder\"]\n",
    "eval_folder = config[\"eval_folder\"]\n",
    "fixed_time_steps = config[\"fixed_time_steps\"]\n",
    "pca_dim = 512  # 降维目标维度\n",
    "batch_size = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88LrpxNjiXW9"
   },
   "outputs": [],
   "source": [
    "def slide_split_get_label_from_filename(filename):\n",
    "    # print(label_mapping.get(filename[9:-5],-1))\n",
    "    return label_mapping.get(filename[9:-5],-1)\n",
    "\n",
    "def split_get_label_from_filename(filename):\n",
    "    # print(label_mapping.get(filename[8:-5],-1))\n",
    "    return label_mapping.get(filename[8:-5],-1)\n",
    "\n",
    "def get_label_from_filename(filename):\n",
    "    match = re.search(r'_(\\d+_?\\d*)\\.json$', filename)\n",
    "    if match:\n",
    "        label_key = match.group(1)\n",
    "        return label_mapping.get(label_key, -1)\n",
    "\n",
    "# 获取文件夹中的文件及其标签\n",
    "def load_files_and_labels(folder_path,split=0):\n",
    "    file_list = []\n",
    "    labels = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        # print(\"file: \", file)\n",
    "        if file.endswith(\".json\"):\n",
    "            if split == 0:\n",
    "                label = get_label_from_filename(file)\n",
    "            elif split == 4:\n",
    "                label = slide_split_get_label_from_filename(file)\n",
    "            elif split==1 or split==2 or split==3:\n",
    "                label = split_get_label_from_filename(file)\n",
    "            if label != -1:\n",
    "                file_list.append(os.path.join(folder_path, file))\n",
    "                labels.append(label)\n",
    "    return file_list, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "Do1IExw8M9s0"
   },
   "outputs": [],
   "source": [
    "# 1. 提取字符并编码为整数索引\n",
    "def hex_to_sequence(hex_feature):\n",
    "    \"\"\"\n",
    "    将十六进制字符串转换为整数索引序列。\n",
    "    去掉冒号并转换为字符对应的索引。\n",
    "    \"\"\"\n",
    "    hex_chars = hex_feature.replace(\":\", \"\")\n",
    "    char_to_index = {char: idx for idx, char in enumerate(\"0123456789abcdef\")}\n",
    "    return [char_to_index[char] for char in hex_chars]\n",
    "\n",
    "def process_json(file_path):\n",
    "    \"\"\"\n",
    "    处理 JSON 文件，提取时间戳、包长度和 raw data 特征，并对特征进行归一化。\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    try:\n",
    "        # 打开 JSON 文件\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            datas = json.load(file)\n",
    "\n",
    "        # 遍历 JSON 数据，提取时间戳和包长度\n",
    "        initial_timestamp = None\n",
    "        pre_time=None\n",
    "        relative_timestamp = None\n",
    "        features_1 = []\n",
    "        features_2 = []\n",
    "        for data in datas:\n",
    "            try:\n",
    "                timestamp = float(data[\"_source\"][\"layers\"][\"frame\"][\"frame.time_relative\"])  # 时间戳\n",
    "                packet_length = int(data[\"_source\"][\"layers\"][\"frame\"][\"frame.len\"])  # 包长度\n",
    "\n",
    "                # 获取数据部分，若不存在则为0\n",
    "                if 'data' in data[\"_source\"][\"layers\"]:\n",
    "                    rawdata = data[\"_source\"][\"layers\"][\"data\"][\"data.data\"]\n",
    "                else:\n",
    "                    rawdata = '0'\n",
    "\n",
    "                # 将原始数据转换为整数序列\n",
    "                data_feature = hex_to_sequence(rawdata)\n",
    "\n",
    "                # 将数据填充或截断为指定长度 length\n",
    "\n",
    "\n",
    "                if len(data_feature) < 2832:  # 如果长度小于 2832，填充 0\n",
    "                    data_feature += [0] * (2832 - len(data_feature))\n",
    "                elif len(data_feature) > 2832:  # 如果长度大于 2832，截断\n",
    "                    data_feature = data_feature[:2832]\n",
    "\n",
    "                # 如果数据长度不符合预期，打印出来\n",
    "                if len(data_feature) != 2832:\n",
    "                    print(f\"Data feature length mismatch: {len(data_feature)}\")\n",
    "\n",
    "                # 初始化时间戳\n",
    "                if initial_timestamp is None:\n",
    "                    initial_timestamp = timestamp  # 设置初始时间戳\n",
    "\n",
    "                # 计算相对时间戳\n",
    "\n",
    "                relative_timestamp = timestamp - initial_timestamp\n",
    "                if pre_time==None:\n",
    "                    pre_time=relative_timestamp\n",
    "                times_diff=relative_timestamp-pre_time\n",
    "                pre_time=relative_timestamp\n",
    "\n",
    "                timestamp_array = np.array([relative_timestamp], dtype=float)\n",
    "                time_diff_array = np.array([times_diff], dtype=float)\n",
    "                packet_length_array = np.array([packet_length], dtype=float)/1512#length 归一化\n",
    "                data_feature = np.array(data_feature, dtype=float)/15\n",
    "\n",
    "                # 将特征按顺序组合为 [时间戳,time diff, 包长度, ]\n",
    "                feature_1 = np.hstack((timestamp_array,time_diff_array, packet_length_array))# ,data_feature\n",
    "\n",
    "                # 添加到特征列表\n",
    "                features_1.append(feature_1)\n",
    "                features_2.append(data_feature)\n",
    "\n",
    "            except (KeyError, ValueError) as e:\n",
    "                # 跳过有问题的数据包\n",
    "                print(f\"Skipping packet due to error: {e}\")\n",
    "                continue\n",
    "\n",
    "        features_1_array=np.array(features_1)\n",
    "        features_2_array=np.array(features_2)\n",
    "        max_timestamp = np.max(features_1_array[:, 0])  # 获取最大时间戳\n",
    "        # print(\"max\")\n",
    "        features_1_array[:, 0] = [feature[0] / max_timestamp for feature in features_1_array]  # 时间戳归一化\n",
    "        max_time_diff=np.max(features_1_array[:, 1])\n",
    "        features_1_array[:, 1] = [feature[1] / max_time_diff for feature in features_1_array]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(Exception)\n",
    "        print(f\"Error processing file {file_path}:s {e}\")\n",
    "        features_1_array = np.zeros((1, 3))  # 返回空特征以避免程序中断\n",
    "        features_2_array = np.zeros((1, 2832))\n",
    "\n",
    "    return features_1_array,features_2_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "RoWOx_3o6SLY"
   },
   "outputs": [],
   "source": [
    "## 数据加载\n",
    "def load_data(folder_path, fixed_time_steps, pca_dim):\n",
    "    file_paths, labels = load_files_and_labels(folder_path,split)\n",
    "    features_1_list, features_2_list = [], []\n",
    "\n",
    "    print(\"processing json\")\n",
    "    for file_path in file_paths:\n",
    "        f1, f2 = process_json(file_path)\n",
    "        features_1_list.append(f1)\n",
    "        features_2_list.append(f2)\n",
    "\n",
    "    print(\"lowing dimensions\")\n",
    "    # PCA 降维\n",
    "    pca = PCA(n_components=pca_dim)\n",
    "    reduced_features_2_list = [pca.fit_transform(f2) for f2 in features_2_list]\n",
    "\n",
    "    combined_features = []\n",
    "    for f1, f2 in zip(features_1_list, reduced_features_2_list):\n",
    "        combined = np.hstack((f1, f2))\n",
    "        if combined.shape[0] > fixed_time_steps:\n",
    "            combined = combined[:fixed_time_steps, :]\n",
    "        else:\n",
    "            combined = np.pad(combined, ((0, fixed_time_steps - combined.shape[0]), (0, 0)), mode='constant')\n",
    "        combined_features.append(combined)\n",
    "\n",
    "    features_tensor = torch.tensor(np.stack(combined_features, axis=0), dtype=torch.float32)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    print(f\"生成了 {len(features_1_list)} 个样本特征\")\n",
    "    print(f\"第一个样本的特征1形状: {features_1_list[0].shape}\")\n",
    "    print(f\"第一个样本的特征2形状: {features_2_list[0].shape}\")\n",
    "    return features_tensor, labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cPRuNVDkje-p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here\n",
      "2\n",
      "Here\n",
      "1\n",
      "Here\n",
      "1\n",
      "Here\n",
      "1\n",
      "Here\n",
      "2\n",
      "Here\n",
      "3\n",
      "Here\n",
      "2\n",
      "Here\n",
      "2\n",
      "Here\n",
      "0\n",
      "Here\n",
      "3\n",
      "Here\n",
      "0\n",
      "Here\n",
      "3\n",
      "Here\n",
      "0\n",
      "Here\n",
      "1\n",
      "Here\n",
      "2\n",
      "Here\n",
      "3\n",
      "Here\n",
      "3\n",
      "Here\n",
      "0\n",
      "Here\n",
      "3\n",
      "Here\n",
      "3\n",
      "Here\n",
      "1\n",
      "Here\n",
      "3\n",
      "Here\n",
      "3\n",
      "Here\n",
      "1\n",
      "Here\n",
      "2\n",
      "Here\n",
      "2\n",
      "Here\n",
      "3\n",
      "Here\n",
      "0\n",
      "Here\n",
      "2\n",
      "Here\n",
      "0\n",
      "Here\n",
      "0\n",
      "Here\n",
      "3\n",
      "Here\n",
      "2\n",
      "Here\n",
      "1\n",
      "Here\n",
      "2\n",
      "Here\n",
      "0\n",
      "Here\n",
      "1\n",
      "Here\n",
      "0\n",
      "Here\n",
      "0\n",
      "Here\n",
      "0\n",
      "Here\n",
      "3\n",
      "Here\n",
      "3\n",
      "Here\n",
      "3\n",
      "Here\n",
      "3\n",
      "Here\n",
      "2\n",
      "Here\n",
      "1\n",
      "Here\n",
      "3\n",
      "Here\n",
      "2\n",
      "Here\n",
      "1\n",
      "Here\n",
      "0\n",
      "Here\n",
      "2\n",
      "Here\n",
      "1\n",
      "Here\n",
      "1\n",
      "Here\n",
      "3\n",
      "Here\n",
      "3\n",
      "Here\n",
      "1\n",
      "Here\n",
      "2\n",
      "Here\n",
      "2\n",
      "Here\n",
      "1\n",
      "Here\n",
      "2\n",
      "Here\n",
      "0\n",
      "Here\n",
      "0\n",
      "Here\n",
      "0\n",
      "Here\n",
      "2\n",
      "Here\n",
      "3\n",
      "Here\n",
      "2\n",
      "Here\n",
      "1\n",
      "Here\n",
      "3\n",
      "Here\n",
      "1\n",
      "Here\n",
      "0\n",
      "Here\n",
      "0\n",
      "Here\n",
      "1\n",
      "Here\n",
      "0\n",
      "Here\n",
      "2\n",
      "Here\n",
      "2\n",
      "Here\n",
      "0\n",
      "Here\n",
      "2\n",
      "Here\n",
      "0\n",
      "Here\n",
      "2\n",
      "Here\n",
      "0\n",
      "Here\n",
      "0\n",
      "Here\n",
      "1\n",
      "Here\n",
      "0\n",
      "Here\n",
      "2\n",
      "Here\n",
      "3\n",
      "Here\n",
      "2\n",
      "Here\n",
      "0\n",
      "Here\n",
      "0\n",
      "Here\n",
      "3\n",
      "Here\n",
      "2\n",
      "Here\n",
      "1\n",
      "Here\n",
      "0\n",
      "Here\n",
      "0\n",
      "Here\n",
      "3\n",
      "Here\n",
      "1\n",
      "Here\n",
      "0\n",
      "Here\n",
      "0\n",
      "Here\n",
      "1\n",
      "Here\n",
      "1\n",
      "Here\n",
      "3\n",
      "Here\n",
      "1\n",
      "Here\n",
      "0\n",
      "Here\n",
      "3\n",
      "Here\n",
      "3\n",
      "Here\n",
      "1\n",
      "Here\n",
      "2\n",
      "Here\n",
      "3\n",
      "Here\n",
      "3\n",
      "Here\n",
      "2\n",
      "Here\n",
      "1\n",
      "Here\n",
      "2\n",
      "Here\n",
      "1\n",
      "Here\n",
      "0\n",
      "Here\n",
      "0\n",
      "Here\n",
      "0\n",
      "Here\n",
      "0\n",
      "Here\n",
      "0\n",
      "Here\n",
      "0\n",
      "Here\n",
      "3\n",
      "Here\n",
      "2\n",
      "Here\n",
      "2\n",
      "Here\n",
      "3\n",
      "Here\n",
      "2\n",
      "Here\n",
      "3\n",
      "Here\n",
      "2\n",
      "Here\n",
      "1\n",
      "Here\n",
      "3\n",
      "Here\n",
      "1\n",
      "Here\n",
      "2\n",
      "Here\n",
      "1\n",
      "Here\n",
      "2\n",
      "Here\n",
      "1\n",
      "Here\n",
      "2\n",
      "Here\n",
      "3\n",
      "Here\n",
      "0\n",
      "Here\n",
      "3\n",
      "Here\n",
      "2\n",
      "Here\n",
      "2\n",
      "Here\n",
      "1\n",
      "Here\n",
      "1\n",
      "Here\n",
      "0\n",
      "Here\n",
      "2\n",
      "Here\n",
      "0\n",
      "Here\n",
      "3\n",
      "Here\n",
      "3\n",
      "Here\n",
      "1\n",
      "Here\n",
      "1\n",
      "Here\n",
      "2\n",
      "Here\n",
      "1\n",
      "Here\n",
      "3\n",
      "Here\n",
      "3\n",
      "Here\n",
      "3\n",
      "Here\n",
      "2\n",
      "Here\n",
      "1\n",
      "Here\n",
      "2\n",
      "Here\n",
      "3\n",
      "Here\n",
      "3\n",
      "Here\n",
      "0\n",
      "Here\n",
      "0\n",
      "Here\n",
      "1\n",
      "Here\n",
      "3\n",
      "Here\n",
      "1\n",
      "Here\n",
      "2\n",
      "Here\n",
      "0\n",
      "Here\n",
      "3\n",
      "Here\n",
      "2\n",
      "Here\n",
      "0\n",
      "Here\n",
      "0\n",
      "Here\n",
      "1\n",
      "Here\n",
      "2\n",
      "Here\n",
      "0\n",
      "Here\n",
      "1\n",
      "Here\n",
      "2\n",
      "Here\n",
      "1\n",
      "Here\n",
      "2\n",
      "Here\n",
      "2\n",
      "Here\n",
      "3\n",
      "Here\n",
      "1\n",
      "Here\n",
      "1\n",
      "Here\n",
      "2\n",
      "Here\n",
      "1\n",
      "Here\n",
      "0\n",
      "Here\n",
      "3\n",
      "Here\n",
      "0\n",
      "Here\n",
      "0\n",
      "Here\n",
      "3\n",
      "Here\n",
      "1\n",
      "Here\n",
      "0\n",
      "Here\n",
      "1\n",
      "Here\n",
      "1\n",
      "Here\n",
      "1\n",
      "Here\n",
      "0\n",
      "Here\n",
      "3\n",
      "Here\n",
      "3\n",
      "Here\n",
      "1\n",
      "Here\n",
      "0\n",
      "Here\n",
      "3\n",
      "Here\n",
      "2\n",
      "Here\n",
      "2\n",
      "Here\n",
      "1\n",
      "Here\n",
      "3\n",
      "Here\n",
      "0\n",
      "Here\n",
      "2\n",
      "Here\n",
      "3\n",
      "Here\n",
      "1\n",
      "Here\n",
      "3\n",
      "Here\n",
      "1\n",
      "Here\n",
      "2\n",
      "processing json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f0de81e0e50>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nesl/anaconda3/envs/202_proj/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "train_features, train_labels = load_data(train_folder, fixed_time_steps, pca_dim)\n",
    "eval_features, eval_labels = load_data(eval_folder, fixed_time_steps, pca_dim)\n",
    "\n",
    "# 数据加载器\n",
    "train_loader = DataLoader(TensorDataset(train_features, train_labels), batch_size=batch_size, shuffle=True)\n",
    "eval_loader = DataLoader(TensorDataset(eval_features, eval_labels), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 神经网络可调参数\n",
    "hidden_size = 16\n",
    "num_layers=1\n",
    "dropout=0.4\n",
    "learning_rate=0.001\n",
    "num_epochs = 1000\n",
    "best_accuracy = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8odv4g-nkD4p"
   },
   "outputs": [],
   "source": [
    "# 模型定义\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=2, dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nKpq25lLjrZr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "Train Loss: 1.4214, Train Accuracy: 5.77%\n",
      "Eval Loss: 1.3935, Eval Accuracy: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nesl/anaconda3/envs/202_proj/lib/python3.9/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000\n",
      "Train Loss: 1.3456, Train Accuracy: 13.46%\n",
      "Eval Loss: 1.3660, Eval Accuracy: 13.04%\n",
      "Epoch 3/1000\n",
      "Train Loss: 1.2930, Train Accuracy: 32.69%\n",
      "Eval Loss: 1.3320, Eval Accuracy: 13.04%\n",
      "Model saved with Accuracy: Traning--57.69% and Eval--43.48%\n",
      "Epoch 4/1000\n",
      "Train Loss: 1.2554, Train Accuracy: 57.69%\n",
      "Eval Loss: 1.2917, Eval Accuracy: 43.48%\n",
      "Model saved with Accuracy: Traning--84.62% and Eval--60.87%\n",
      "Epoch 5/1000\n",
      "Train Loss: 1.2056, Train Accuracy: 84.62%\n",
      "Eval Loss: 1.2642, Eval Accuracy: 60.87%\n",
      "Model saved with Accuracy: Traning--98.08% and Eval--65.22%\n",
      "Epoch 6/1000\n",
      "Train Loss: 1.1599, Train Accuracy: 98.08%\n",
      "Eval Loss: 1.2209, Eval Accuracy: 65.22%\n",
      "Model saved with Accuracy: Traning--100.00% and Eval--78.26%\n",
      "Epoch 7/1000\n",
      "Train Loss: 1.1009, Train Accuracy: 100.00%\n",
      "Eval Loss: 1.2085, Eval Accuracy: 78.26%\n",
      "Model saved with Accuracy: Traning--100.00% and Eval--82.61%\n",
      "Epoch 8/1000\n",
      "Train Loss: 1.0790, Train Accuracy: 100.00%\n",
      "Eval Loss: 1.1708, Eval Accuracy: 82.61%\n",
      "Epoch 9/1000\n",
      "Train Loss: 1.0141, Train Accuracy: 100.00%\n",
      "Eval Loss: 1.1231, Eval Accuracy: 82.61%\n",
      "Model saved with Accuracy: Traning--100.00% and Eval--91.30%\n",
      "Epoch 10/1000\n",
      "Train Loss: 0.9647, Train Accuracy: 100.00%\n",
      "Eval Loss: 1.0962, Eval Accuracy: 91.30%\n",
      "Epoch 11/1000\n",
      "Train Loss: 0.9074, Train Accuracy: 100.00%\n",
      "Eval Loss: 1.0826, Eval Accuracy: 91.30%\n",
      "Epoch 12/1000\n",
      "Train Loss: 0.8815, Train Accuracy: 100.00%\n",
      "Eval Loss: 1.0446, Eval Accuracy: 91.30%\n",
      "Epoch 13/1000\n",
      "Train Loss: 0.8515, Train Accuracy: 100.00%\n",
      "Eval Loss: 1.0242, Eval Accuracy: 91.30%\n",
      "Model saved with Accuracy: Traning--100.00% and Eval--95.65%\n",
      "Epoch 14/1000\n",
      "Train Loss: 0.8101, Train Accuracy: 100.00%\n",
      "Eval Loss: 1.0009, Eval Accuracy: 95.65%\n",
      "Epoch 15/1000\n",
      "Train Loss: 0.7416, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.9353, Eval Accuracy: 95.65%\n",
      "Epoch 16/1000\n",
      "Train Loss: 0.7199, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.9220, Eval Accuracy: 95.65%\n",
      "Epoch 17/1000\n",
      "Train Loss: 0.6823, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.8666, Eval Accuracy: 95.65%\n",
      "Epoch 18/1000\n",
      "Train Loss: 0.6288, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.8463, Eval Accuracy: 95.65%\n",
      "Model saved with Accuracy: Traning--100.00% and Eval--100.00%\n",
      "Epoch 19/1000\n",
      "Train Loss: 0.5692, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.8253, Eval Accuracy: 100.00%\n",
      "Epoch 20/1000\n",
      "Train Loss: 0.5082, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.7699, Eval Accuracy: 100.00%\n",
      "Epoch 21/1000\n",
      "Train Loss: 0.4711, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.7621, Eval Accuracy: 100.00%\n",
      "Epoch 22/1000\n",
      "Train Loss: 0.4103, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.6966, Eval Accuracy: 100.00%\n",
      "Epoch 23/1000\n",
      "Train Loss: 0.3729, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.6474, Eval Accuracy: 100.00%\n",
      "Epoch 24/1000\n",
      "Train Loss: 0.3252, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.6209, Eval Accuracy: 100.00%\n",
      "Epoch 25/1000\n",
      "Train Loss: 0.2922, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.5736, Eval Accuracy: 100.00%\n",
      "Epoch 26/1000\n",
      "Train Loss: 0.2609, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.5529, Eval Accuracy: 100.00%\n",
      "Epoch 27/1000\n",
      "Train Loss: 0.2423, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.5286, Eval Accuracy: 100.00%\n",
      "Epoch 28/1000\n",
      "Train Loss: 0.2186, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.4888, Eval Accuracy: 100.00%\n",
      "Epoch 29/1000\n",
      "Train Loss: 0.1916, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.4189, Eval Accuracy: 100.00%\n",
      "Epoch 30/1000\n",
      "Train Loss: 0.1534, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.4224, Eval Accuracy: 100.00%\n",
      "Epoch 31/1000\n",
      "Train Loss: 0.1592, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.3610, Eval Accuracy: 100.00%\n",
      "Epoch 32/1000\n",
      "Train Loss: 0.1312, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.3492, Eval Accuracy: 100.00%\n",
      "Epoch 33/1000\n",
      "Train Loss: 0.1233, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.3366, Eval Accuracy: 100.00%\n",
      "Epoch 34/1000\n",
      "Train Loss: 0.1159, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.3172, Eval Accuracy: 100.00%\n",
      "Epoch 35/1000\n",
      "Train Loss: 0.0992, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.2711, Eval Accuracy: 100.00%\n",
      "Epoch 36/1000\n",
      "Train Loss: 0.0919, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.2691, Eval Accuracy: 100.00%\n",
      "Epoch 37/1000\n",
      "Train Loss: 0.0802, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.2637, Eval Accuracy: 100.00%\n",
      "Epoch 38/1000\n",
      "Train Loss: 0.0732, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.2694, Eval Accuracy: 100.00%\n",
      "Epoch 39/1000\n",
      "Train Loss: 0.0725, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.2279, Eval Accuracy: 100.00%\n",
      "Epoch 40/1000\n",
      "Train Loss: 0.0635, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.2246, Eval Accuracy: 100.00%\n",
      "Epoch 41/1000\n",
      "Train Loss: 0.0622, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.1869, Eval Accuracy: 100.00%\n",
      "Epoch 42/1000\n",
      "Train Loss: 0.0568, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.1640, Eval Accuracy: 100.00%\n",
      "Epoch 43/1000\n",
      "Train Loss: 0.0512, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.1618, Eval Accuracy: 100.00%\n",
      "Epoch 44/1000\n",
      "Train Loss: 0.0457, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.1474, Eval Accuracy: 100.00%\n",
      "Epoch 45/1000\n",
      "Train Loss: 0.0395, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.1322, Eval Accuracy: 100.00%\n",
      "Epoch 46/1000\n",
      "Train Loss: 0.0388, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.1443, Eval Accuracy: 100.00%\n",
      "Epoch 47/1000\n",
      "Train Loss: 0.0351, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.1143, Eval Accuracy: 100.00%\n",
      "Epoch 48/1000\n",
      "Train Loss: 0.0337, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.1164, Eval Accuracy: 100.00%\n",
      "Epoch 49/1000\n",
      "Train Loss: 0.0326, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.0928, Eval Accuracy: 100.00%\n",
      "Epoch 50/1000\n",
      "Train Loss: 0.0299, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.0924, Eval Accuracy: 100.00%\n",
      "Epoch 51/1000\n",
      "Train Loss: 0.0288, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.0832, Eval Accuracy: 100.00%\n",
      "Epoch 52/1000\n",
      "Train Loss: 0.0259, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.0819, Eval Accuracy: 100.00%\n",
      "Epoch 53/1000\n",
      "Train Loss: 0.0251, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.0893, Eval Accuracy: 100.00%\n",
      "Epoch 54/1000\n",
      "Train Loss: 0.0235, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.0813, Eval Accuracy: 100.00%\n",
      "Epoch 55/1000\n",
      "Train Loss: 0.0229, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.0699, Eval Accuracy: 100.00%\n",
      "Epoch 56/1000\n",
      "Train Loss: 0.0213, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.0615, Eval Accuracy: 100.00%\n",
      "Epoch 57/1000\n",
      "Train Loss: 0.0204, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.0542, Eval Accuracy: 100.00%\n",
      "Epoch 58/1000\n",
      "Train Loss: 0.0196, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.0532, Eval Accuracy: 100.00%\n",
      "Epoch 59/1000\n",
      "Train Loss: 0.0194, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.0484, Eval Accuracy: 100.00%\n",
      "Epoch 60/1000\n",
      "Train Loss: 0.0178, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.0468, Eval Accuracy: 100.00%\n",
      "Epoch 61/1000\n",
      "Train Loss: 0.0161, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.0453, Eval Accuracy: 100.00%\n",
      "Epoch 62/1000\n",
      "Train Loss: 0.0160, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.0416, Eval Accuracy: 100.00%\n",
      "Epoch 63/1000\n",
      "Train Loss: 0.0155, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.0412, Eval Accuracy: 100.00%\n",
      "Epoch 64/1000\n",
      "Train Loss: 0.0150, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.0381, Eval Accuracy: 100.00%\n",
      "Epoch 65/1000\n",
      "Train Loss: 0.0144, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.0400, Eval Accuracy: 100.00%\n",
      "Epoch 66/1000\n",
      "Train Loss: 0.0145, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.0367, Eval Accuracy: 100.00%\n",
      "Epoch 67/1000\n",
      "Train Loss: 0.0146, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.0333, Eval Accuracy: 100.00%\n",
      "Epoch 68/1000\n",
      "Train Loss: 0.0141, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.0383, Eval Accuracy: 100.00%\n",
      "Epoch 69/1000\n",
      "Train Loss: 0.0134, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.0354, Eval Accuracy: 100.00%\n",
      "Epoch 70/1000\n",
      "Train Loss: 0.0128, Train Accuracy: 100.00%\n",
      "Eval Loss: 0.0314, Eval Accuracy: 100.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m total_loss, correct_preds, total_preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m---> 20\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     21\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     22\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output_size = 4\n",
    "input_size = pca_dim + 3\n",
    "model = LSTMModel(input_size, hidden_size, output_size, num_layers=num_layers, dropout=dropout).to(device)\n",
    "\n",
    "# 损失函数与优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=5)\n",
    "\n",
    "# TensorBoard 日志\n",
    "writer = SummaryWriter('runs/lstm_pca')\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss, correct_preds, total_preds = 0, 0, 0\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        correct_preds += (outputs.argmax(1) == targets).sum().item()\n",
    "        total_preds += targets.size(0)\n",
    "\n",
    "    train_accuracy = correct_preds / total_preds\n",
    "    train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        eval_loss, correct_preds, total_preds = 0, 0, 0\n",
    "        for inputs, targets in eval_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            eval_loss += criterion(outputs, targets).item()\n",
    "            correct_preds += (outputs.argmax(1) == targets).sum().item()\n",
    "            total_preds += targets.size(0)\n",
    "\n",
    "    eval_accuracy = correct_preds / total_preds\n",
    "    eval_loss /= len(eval_loader)\n",
    "    scheduler.step(eval_loss)\n",
    "\n",
    "    if eval_accuracy > best_accuracy:\n",
    "        best_accuracy = eval_accuracy\n",
    "        model_filename = f\"model_epoch_{epoch+1}tran_acc_{train_accuracy*100:.2f}% val_acc_{eval_accuracy*100:.2f}%.pth\"\n",
    "        torch.save(model.state_dict(), model_filename)  # Save model to disko disk\n",
    "        print(f\"Model saved with Accuracy: Traning--{train_accuracy*100:.2f}% and Eval--{eval_accuracy*100:.2f}%\")\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "    print(f\"Eval Loss: {eval_loss:.4f}, Eval Accuracy: {eval_accuracy * 100:.2f}%\")\n",
    "    writer.add_scalars('Loss', {'Train': train_loss, 'Eval': eval_loss}, epoch)\n",
    "    writer.add_scalars('Accuracy', {'Train': train_accuracy, 'Eval': eval_accuracy}, epoch)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DjNJBaXOgMd0",
    "outputId": "1c441502-2a33-4fb6-dfce-6273f2846020"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "True Labels:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[]\n",
      "Test Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "pretrain=True\n",
    "model_path=\"model_epoch_24tran_acc_100.00% val_acc_100.00%.pth\"\n",
    "if pretrain:\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_preds = []  # To store predictions\n",
    "    test_labels = []  # To store true labels\n",
    "\n",
    "    # Loop through the train_loader (or test_loader if you're evaluating the test set)\n",
    "    for inputs, targets in train_loader:\n",
    "        # Move inputs and targets to the same device as the model\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Ensure the input shape is (batch_size, seq_len, input_size) for LSTM\n",
    "        # Adjust this depending on the actual input shape\n",
    "        # inputs = inputs.view(inputs.size(0), -1, inputs.size(1))  # Assuming inputs have shape (batch_size, input_size)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Get predictions by taking the argmax along the output dimension\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Store predictions and true labels as NumPy arrays\n",
    "        test_preds.extend(predicted.cpu().numpy())\n",
    "        test_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "        # Print predictions and labels for debugging (optional)\n",
    "    print(\"Predictions: \", test_preds)\n",
    "    print(\"True Labels: \", test_labels)\n",
    "\n",
    "    # You may want to calculate accuracy or other metrics after the loop\n",
    "    # For example, calculate accuracy:\n",
    "\n",
    "    accuracy = sum(np.array(test_preds) == np.array(test_labels)) / len(test_labels)\n",
    "    # print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    diff_indices = [i for i in range(len(test_labels)) if test_labels[i] != test_preds[i]]\n",
    "\n",
    "    print(diff_indices)\n",
    "\n",
    "\n",
    "\n",
    "    # accuracy = accuracy_score(test_labels, test_preds)\n",
    "    print(f\"Test Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yGJZUDKQNvDd",
    "outputId": "30b8fe9c-b921-4239-d765-15eb4dab8c2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Metrics:\n",
      "Accuracy: 0.89\n",
      "Precision (Macro): 0.92\n",
      "Recall (Macro/Sensitivity): 0.89\n",
      "\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        Static       1.00      0.83      0.91        18\n",
      " Slightly Move       1.00      0.93      0.97        15\n",
      "          Move       0.69      1.00      0.82        18\n",
      "Intensely Move       1.00      0.79      0.88        19\n",
      "\n",
      "      accuracy                           0.89        70\n",
      "     macro avg       0.92      0.89      0.89        70\n",
      "  weighted avg       0.92      0.89      0.89        70\n",
      "\n",
      "\n",
      "Validation Set Metrics:\n",
      "Accuracy: 0.45\n",
      "Precision (Macro): 0.46\n",
      "Recall (Macro/Sensitivity): 0.46\n",
      "\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        Static       0.45      0.71      0.56         7\n",
      " Slightly Move       0.50      0.30      0.38        10\n",
      "          Move       0.60      0.43      0.50         7\n",
      "Intensely Move       0.29      0.40      0.33         5\n",
      "\n",
      "      accuracy                           0.45        29\n",
      "     macro avg       0.46      0.46      0.44        29\n",
      "  weighted avg       0.48      0.45      0.44        29\n",
      "\n",
      "\n",
      "Overall Metrics:\n",
      "Accuracy: 0.76\n",
      "Precision (Macro): 0.77\n",
      "Recall (Macro/Sensitivity): 0.76\n",
      "\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        Static       0.77      0.80      0.78        25\n",
      " Slightly Move       0.85      0.68      0.76        25\n",
      "          Move       0.68      0.84      0.75        25\n",
      "Intensely Move       0.77      0.71      0.74        24\n",
      "\n",
      "      accuracy                           0.76        99\n",
      "     macro avg       0.77      0.76      0.76        99\n",
      "  weighted avg       0.77      0.76      0.76        99\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, class_names):\n",
    "    \"\"\"\n",
    "    Calculate and display confusion matrix and metrics.\n",
    "    :param y_true: Ground truth labels.\n",
    "    :param y_pred: Predicted labels.\n",
    "    :param class_names: List of class names.\n",
    "    \"\"\"\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='macro')  # Macro-average for multiclass\n",
    "    recall = recall_score(y_true, y_pred, average='macro')        # Macro-average for multiclass\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision (Macro): {precision:.2f}\")\n",
    "    print(f\"Recall (Macro/Sensitivity): {recall:.2f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "# Gather predictions and labels for training set\n",
    "train_preds, train_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_preds.extend(predicted.cpu().numpy())\n",
    "        train_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "# Gather predictions and labels for validation set\n",
    "eval_preds, eval_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in eval_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        eval_preds.extend(predicted.cpu().numpy())\n",
    "        eval_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "# Combine training and validation predictions for overall metrics\n",
    "combined_preds = train_preds + eval_preds\n",
    "combined_labels = train_labels + eval_labels\n",
    "\n",
    "# Define class names for display\n",
    "class_names = [\"Static\", \"Slightly Move\", \"Move\", \"Intensely Move\"]\n",
    "\n",
    "# Calculate and display metrics for training set\n",
    "print(\"Training Set Metrics:\")\n",
    "calculate_metrics(train_labels, train_preds, class_names)\n",
    "\n",
    "# Calculate and display metrics for validation set\n",
    "print(\"\\nValidation Set Metrics:\")\n",
    "calculate_metrics(eval_labels, eval_preds, class_names)\n",
    "\n",
    "# Calculate and display metrics for combined data\n",
    "print(\"\\nOverall Metrics:\")\n",
    "calculate_metrics(combined_labels, combined_preds, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gm3nQwrmNvFm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1u9r3Ks1vpGTKk8Y8fmT7pWWTRKB6kU0B",
     "timestamp": 1733522840429
    }
   ]
  },
  "kernelspec": {
   "display_name": "202_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
