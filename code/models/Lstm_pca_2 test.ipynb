{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YGw4jevKM2We"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4Uss33YfM9qV"
   },
   "outputs": [],
   "source": [
    "def slide_split_get_label_from_filename(filename):\n",
    "    # print(label_mapping.get(filename[9:-5],-1))\n",
    "    return label_mapping.get(filename[9:-5],-1)\n",
    "\n",
    "def split_get_label_from_filename(filename):\n",
    "    return label_mapping.get(filename[8:-5],-1)\n",
    "\n",
    "def get_label_from_filename(filename):\n",
    "    match = re.search(r'_(\\d+_?\\d*)\\.json$', filename)\n",
    "    print(match)\n",
    "    if match:\n",
    "        label_key = match.group(1)\n",
    "        return label_mapping.get(label_key, -1)  # 如果标签不匹配返回 -1\n",
    "\n",
    "# 获取文件夹中的文件及其标签\n",
    "def load_files_and_labels(folder_path,split=False):\n",
    "    file_list = []\n",
    "    labels = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        # print(\"file: \", file)\n",
    "        if file.endswith(\".json\"):\n",
    "            if split == 1 or split==2 or split==3:\n",
    "                label = split_get_label_from_filename(file)\n",
    "            elif split == 4:\n",
    "                # print(f\"split is {split}\")\n",
    "                label = slide_split_get_label_from_filename(file)\n",
    "            else:\n",
    "                label = get_label_from_filename(file)\n",
    "            if label != -1:\n",
    "                file_list.append(os.path.join(folder_path, file))\n",
    "                labels.append(label)\n",
    "            #print(f\"File: {file}, Label: {label}\")\n",
    "\n",
    "    return file_list, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Do1IExw8M9s0"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# 1. 提取字符并编码为整数索引\n",
    "def hex_to_sequence(hex_feature):\n",
    "    \"\"\"\n",
    "    将十六进制字符串转换为整数索引序列。\n",
    "    去掉冒号并转换为字符对应的索引。\n",
    "    \"\"\"\n",
    "    hex_chars = hex_feature.replace(\":\", \"\")\n",
    "    char_to_index = {char: idx for idx, char in enumerate(\"0123456789abcdef\")}\n",
    "    return [char_to_index[char] for char in hex_chars]\n",
    "\n",
    "def process_json(file_path):\n",
    "    \"\"\"\n",
    "    处理 JSON 文件，提取时间戳、包长度和 raw data 特征，并对特征进行归一化。\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    try:\n",
    "        # 打开 JSON 文件\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            datas = json.load(file)\n",
    "\n",
    "        # 遍历 JSON 数据，提取时间戳和包长度\n",
    "        initial_timestamp = None\n",
    "        pre_time=None\n",
    "        relative_timestamp = None\n",
    "        features_1 = []\n",
    "        features_2 = []\n",
    "        for data in datas:\n",
    "            try:\n",
    "                timestamp = float(data[\"_source\"][\"layers\"][\"frame\"][\"frame.time_relative\"])  # 时间戳\n",
    "                packet_length = int(data[\"_source\"][\"layers\"][\"frame\"][\"frame.len\"])  # 包长度\n",
    "\n",
    "                # 获取数据部分，若不存在则为0\n",
    "                if 'data' in data[\"_source\"][\"layers\"]:\n",
    "                    rawdata = data[\"_source\"][\"layers\"][\"data\"][\"data.data\"]\n",
    "                else:\n",
    "                    rawdata = '0'\n",
    "\n",
    "                # 将原始数据转换为整数序列\n",
    "                data_feature = hex_to_sequence(rawdata)\n",
    "\n",
    "                # 将数据填充或截断为指定长度 length\n",
    "\n",
    "\n",
    "                if len(data_feature) < 2832:  # 如果长度小于 2832，填充 0\n",
    "                    data_feature += [0] * (2832 - len(data_feature))\n",
    "                elif len(data_feature) > 2832:  # 如果长度大于 2832，截断\n",
    "                    data_feature = data_feature[:2832]\n",
    "\n",
    "                # 如果数据长度不符合预期，打印出来\n",
    "                if len(data_feature) != 2832:\n",
    "                    print(f\"Data feature length mismatch: {len(data_feature)}\")\n",
    "\n",
    "                # 初始化时间戳\n",
    "                if initial_timestamp is None:\n",
    "                    initial_timestamp = timestamp  # 设置初始时间戳\n",
    "\n",
    "                # 计算相对时间戳\n",
    "                \n",
    "                relative_timestamp = timestamp - initial_timestamp\n",
    "                if pre_time==None:\n",
    "                    pre_time=relative_timestamp\n",
    "                # print(type(relative_timestamp))\n",
    "                times_diff=relative_timestamp-pre_time\n",
    "                pre_time=relative_timestamp\n",
    "\n",
    "\n",
    "\n",
    "                timestamp_array = np.array([relative_timestamp], dtype=float)\n",
    "                time_diff_array = np.array([times_diff], dtype=float)\n",
    "                packet_length_array = np.array([packet_length], dtype=float)/1512#length 归一化\n",
    "                data_feature = np.array(data_feature, dtype=float)/15\n",
    "\n",
    "                # 将特征按顺序组合为 [时间戳,time diff, 包长度, ]\n",
    "                feature_1 = np.hstack((timestamp_array,time_diff_array, packet_length_array))# ,data_feature\n",
    "\n",
    "                # 添加到特征列表\n",
    "                features_1.append(feature_1)\n",
    "                features_2.append(data_feature)\n",
    "\n",
    "            except (KeyError, ValueError) as e:\n",
    "                # 跳过有问题的数据包\n",
    "                print(f\"Skipping packet due to error: {e}\")\n",
    "                continue\n",
    "\n",
    "        features_1_array=np.array(features_1)\n",
    "        features_2_array=np.array(features_2)\n",
    "        # print(f\"features_1_array.shape\",features_1_array.shape)\n",
    "        # print(f\"features_2_array.shape\",features_2_array.shape)\n",
    "        max_timestamp = np.max(features_1_array[:, 0])  # 获取最大时间戳\n",
    "        # print(\"max\")\n",
    "        features_1_array[:, 0] = [feature[0] / max_timestamp for feature in features_1_array]  # 时间戳归一化\n",
    "        max_time_diff=np.max(features_1_array[:, 1])\n",
    "        features_1_array[:, 1] = [feature[1] / max_time_diff for feature in features_1_array]\n",
    "\n",
    "\n",
    "        # print(\"11\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(Exception)\n",
    "        print(f\"Error processing file {file_path}:s {e}\")\n",
    "        features_1_array = np.zeros((1, 3))  # 返回空特征以避免程序中断\n",
    "        features_2_array = np.zeros((1, 2832))\n",
    "\n",
    "    return features_1_array,features_2_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "RoWOx_3o6SLY"
   },
   "outputs": [],
   "source": [
    "split = 0 # 0:100, 1:300, 2:800 3:1000 4:3500\n",
    "# folder_path = \"packet_json_split\"\n",
    "if split==1:\n",
    "    folder_path = \"packet_json_split\"\n",
    "elif split==2:\n",
    "    folder_path = \"202_packet_json_new_800\"\n",
    "elif split==3:\n",
    "    folder_path = \"202_packet_json_new_1000\"\n",
    "elif split==4:\n",
    "    folder_path = \"202_packet_json_new_3500\"\n",
    "else:\n",
    "    folder_path = \"202_packet_json_new\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "31hdu2h-M9vV",
    "outputId": "a9517eb7-98ce-4f85-c8fa-226c75158d88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "<re.Match object; span=(5, 14), match='_0_2.json'>\n",
      "<re.Match object; span=(5, 12), match='_1.json'>\n",
      "<re.Match object; span=(5, 12), match='_1.json'>\n",
      "<re.Match object; span=(5, 12), match='_1.json'>\n",
      "<re.Match object; span=(5, 12), match='_4.json'>\n",
      "<re.Match object; span=(5, 12), match='_4.json'>\n",
      "<re.Match object; span=(5, 12), match='_4.json'>\n",
      "<re.Match object; span=(5, 12), match='_1.json'>\n",
      "<re.Match object; span=(5, 14), match='_0_2.json'>\n",
      "<re.Match object; span=(5, 14), match='_0_2.json'>\n",
      "<re.Match object; span=(5, 14), match='_0_2.json'>\n",
      "<re.Match object; span=(5, 12), match='_4.json'>\n",
      "<re.Match object; span=(5, 12), match='_1.json'>\n",
      "<re.Match object; span=(5, 12), match='_1.json'>\n",
      "<re.Match object; span=(5, 12), match='_0.json'>\n",
      "<re.Match object; span=(5, 12), match='_0.json'>\n",
      "<re.Match object; span=(5, 12), match='_1.json'>\n",
      "<re.Match object; span=(5, 14), match='_0_2.json'>\n",
      "<re.Match object; span=(5, 12), match='_4.json'>\n",
      "<re.Match object; span=(5, 12), match='_0.json'>\n",
      "<re.Match object; span=(5, 12), match='_4.json'>\n",
      "<re.Match object; span=(5, 12), match='_4.json'>\n",
      "<re.Match object; span=(5, 12), match='_4.json'>\n",
      "<re.Match object; span=(5, 12), match='_1.json'>\n",
      "<re.Match object; span=(5, 12), match='_0.json'>\n",
      "<re.Match object; span=(5, 14), match='_0_2.json'>\n",
      "<re.Match object; span=(5, 12), match='_0.json'>\n",
      "<re.Match object; span=(5, 12), match='_1.json'>\n",
      "<re.Match object; span=(5, 12), match='_0.json'>\n",
      "<re.Match object; span=(5, 12), match='_4.json'>\n",
      "<re.Match object; span=(5, 12), match='_0.json'>\n",
      "<re.Match object; span=(5, 12), match='_1.json'>\n",
      "<re.Match object; span=(5, 12), match='_0.json'>\n",
      "<re.Match object; span=(5, 12), match='_0.json'>\n",
      "<re.Match object; span=(5, 12), match='_1.json'>\n",
      "<re.Match object; span=(5, 14), match='_0_2.json'>\n",
      "<re.Match object; span=(5, 12), match='_1.json'>\n",
      "<re.Match object; span=(5, 12), match='_4.json'>\n",
      "<re.Match object; span=(5, 12), match='_0.json'>\n",
      "<re.Match object; span=(5, 14), match='_0_2.json'>\n",
      "<re.Match object; span=(5, 14), match='_0_2.json'>\n",
      "<re.Match object; span=(5, 12), match='_1.json'>\n",
      "<re.Match object; span=(5, 14), match='_0_2.json'>\n",
      "<re.Match object; span=(5, 12), match='_4.json'>\n",
      "<re.Match object; span=(5, 12), match='_0.json'>\n",
      "<re.Match object; span=(5, 12), match='_1.json'>\n",
      "<re.Match object; span=(5, 12), match='_1.json'>\n",
      "<re.Match object; span=(5, 14), match='_0_2.json'>\n",
      "<re.Match object; span=(5, 14), match='_0_2.json'>\n",
      "<re.Match object; span=(5, 12), match='_4.json'>\n",
      "<re.Match object; span=(5, 12), match='_4.json'>\n",
      "<re.Match object; span=(5, 12), match='_4.json'>\n",
      "<re.Match object; span=(5, 12), match='_4.json'>\n",
      "<re.Match object; span=(5, 12), match='_1.json'>\n",
      "<re.Match object; span=(5, 12), match='_1.json'>\n",
      "<re.Match object; span=(5, 12), match='_1.json'>\n",
      "<re.Match object; span=(5, 12), match='_4.json'>\n",
      "<re.Match object; span=(5, 14), match='_0_2.json'>\n",
      "<re.Match object; span=(5, 12), match='_0.json'>\n",
      "<re.Match object; span=(5, 12), match='_0.json'>\n",
      "<re.Match object; span=(5, 12), match='_0.json'>\n",
      "<re.Match object; span=(5, 12), match='_0.json'>\n",
      "<re.Match object; span=(5, 12), match='_4.json'>\n",
      "<re.Match object; span=(5, 12), match='_0.json'>\n",
      "<re.Match object; span=(5, 12), match='_0.json'>\n",
      "<re.Match object; span=(5, 12), match='_0.json'>\n",
      "<re.Match object; span=(5, 12), match='_4.json'>\n",
      "<re.Match object; span=(5, 14), match='_0_2.json'>\n",
      "<re.Match object; span=(5, 12), match='_4.json'>\n",
      "<re.Match object; span=(5, 14), match='_0_2.json'>\n",
      "<re.Match object; span=(5, 12), match='_4.json'>\n",
      "<re.Match object; span=(5, 12), match='_1.json'>\n",
      "<re.Match object; span=(5, 14), match='_0_2.json'>\n",
      "<re.Match object; span=(5, 14), match='_0_2.json'>\n",
      "<re.Match object; span=(5, 14), match='_0_2.json'>\n",
      "<re.Match object; span=(5, 12), match='_4.json'>\n",
      "<re.Match object; span=(5, 12), match='_1.json'>\n",
      "<re.Match object; span=(5, 12), match='_1.json'>\n",
      "<re.Match object; span=(5, 12), match='_0.json'>\n",
      "<re.Match object; span=(5, 12), match='_1.json'>\n",
      "<re.Match object; span=(5, 14), match='_0_2.json'>\n",
      "<re.Match object; span=(5, 12), match='_0.json'>\n",
      "<re.Match object; span=(5, 14), match='_0_2.json'>\n",
      "<re.Match object; span=(5, 14), match='_0_2.json'>\n",
      "<re.Match object; span=(5, 14), match='_0_2.json'>\n",
      "<re.Match object; span=(5, 14), match='_0_2.json'>\n",
      "<re.Match object; span=(5, 12), match='_0.json'>\n",
      "<re.Match object; span=(5, 12), match='_1.json'>\n",
      "<re.Match object; span=(5, 12), match='_1.json'>\n",
      "<re.Match object; span=(5, 14), match='_0_2.json'>\n",
      "<re.Match object; span=(5, 12), match='_0.json'>\n",
      "<re.Match object; span=(5, 12), match='_4.json'>\n",
      "<re.Match object; span=(5, 12), match='_4.json'>\n",
      "<re.Match object; span=(5, 12), match='_0.json'>\n",
      "<re.Match object; span=(5, 14), match='_0_2.json'>\n",
      "<re.Match object; span=(5, 12), match='_0.json'>\n",
      "<re.Match object; span=(5, 12), match='_4.json'>\n",
      "<re.Match object; span=(5, 12), match='_0.json'>\n",
      "<re.Match object; span=(5, 12), match='_1.json'>\n",
      "<re.Match object; span=(5, 12), match='_4.json'>\n",
      "<class 'list'>\n",
      "(100,)\n",
      "[1 2 2 2 3 3 3 2 1 1 1 3 2 2 0 0 2 1 3 0 3 3 3 2 0 1 0 2 0 3 0 2 0 0 2 1 2\n",
      " 3 0 1 1 2 1 3 0 2 2 1 1 3 3 3 3 2 2 2 3 1 0 0 0 0 3 0 0 0 3 1 3 1 3 2 1 1\n",
      " 1 3 2 2 0 2 1 0 1 1 1 1 0 2 2 1 0 3 3 0 1 0 3 0 2 3]\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 定义文件夹路径 /content/drive/MyDrive/202_project/202_packet_json\n",
    "\n",
    "\n",
    "# 定义标签映射\n",
    "label_mapping = {\n",
    "    \"0\": 0,             # static\n",
    "    \"0_2\": 1,           # slightly_move\n",
    "    \"1\": 2,             # move\n",
    "    \"4\": 3              # intensely_move\n",
    "}\n",
    "\n",
    "# 调用函数获取文件和标签\n",
    "data_paths, labels = load_files_and_labels(folder_path,split)\n",
    "# 用于存储每个样本的特征\n",
    "\n",
    "print(type(labels))\n",
    "labels_array=np.array(labels)#[:50]\n",
    "print(labels_array.shape)\n",
    "print(labels_array)\n",
    "labels_tensor=torch.tensor(labels_array,dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GvcsaqOMM9zJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "if split==1:fixed_time_steps = 3000\n",
    "elif split==2: fixed_time_steps = 1600\n",
    "elif split==3: fixed_time_steps = 1200\n",
    "elif split==4: fixed_time_steps = 1600\n",
    "else: fixed_time_steps = 12000\n",
    "# 截断或补零到固定长度\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "YVEDeDN6Aupy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成了 100 个样本特征\n",
      "第一个样本的特征1形状: (13948, 3)\n",
      "第一个样本的特征2形状: (13948, 2832)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "features_1_list = []\n",
    "features_2_list = []\n",
    "\n",
    "# 遍历样本，生成 features_1 和 features_2\n",
    "for data_path in data_paths:\n",
    "    features_1, features_2 = process_json(data_path)\n",
    "    features_1_list.append(features_1)\n",
    "    features_2_list.append(features_2)\n",
    "\n",
    "print(f\"生成了 {len(features_1_list)} 个样本特征\")\n",
    "print(f\"第一个样本的特征1形状: {features_1_list[0].shape}\")\n",
    "print(f\"第一个样本的特征2形状: {features_2_list[0].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "截断后的特征张量形状: torch.Size([100, 12000, 259])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 对特征2降维\n",
    "target_dim = 256  # 降到 256 维\n",
    "pca = PCA(n_components=target_dim)\n",
    "\n",
    "reduced_features_2_list = []\n",
    "for feature in features_2_list:\n",
    "    reduced_feature = pca.fit_transform(feature)  # 对特征2进行PCA降维\n",
    "    reduced_features_2_list.append(reduced_feature)\n",
    "\n",
    "# 确保 features_1_list 和 reduced_features_2_list 的时间步一致\n",
    "for i, (feature1, feature2) in enumerate(zip(features_1_list, reduced_features_2_list)):\n",
    "    if feature1.shape[0] != feature2.shape[0]:\n",
    "        raise ValueError(f\"样本 {i} 的时间步数不一致：feature1 有 {feature1.shape[0]} 步，降维后的 feature2 有 {feature2.shape[0]} 步\")\n",
    "\n",
    "# 横向拼接特征1和降维后的特征2\n",
    "combined_features_list = []\n",
    "for feature1, feature2 in zip(features_1_list, reduced_features_2_list):\n",
    "    combined_feature = np.hstack((feature1, feature2))  # 横向拼接\n",
    "    combined_features_list.append(combined_feature)\n",
    "\n",
    "# 截断或补零到固定时间步\n",
    "aligned_features = []\n",
    "for feature in combined_features_list:\n",
    "    if feature.shape[0] > fixed_time_steps:\n",
    "        truncated = feature[:fixed_time_steps, :]  # 截断\n",
    "    else:\n",
    "        truncated = np.pad(feature, ((0, fixed_time_steps - feature.shape[0]), (0, 0)), mode='constant')  # 补零\n",
    "    aligned_features.append(truncated)\n",
    "\n",
    "# 转换为张量\n",
    "features_tensor = torch.tensor(np.stack(aligned_features, axis=0), dtype=torch.float32)\n",
    "print(f\"截断后的特征张量形状: {features_tensor.shape}\")\n",
    "print(type(features_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "yJt-YqzXM91D",
    "outputId": "97be4e6c-bc89-4796-d92b-d0ec17e8d5b3"
   },
   "outputs": [],
   "source": [
    "if split==1:\n",
    "    train_dataset=TensorDataset(features_tensor[0:210],labels_tensor[0:210])\n",
    "    eval_dataset =TensorDataset(features_tensor[210:-1],labels_tensor[210:-1])\n",
    "elif split==2:\n",
    "    train_dataset=TensorDataset(features_tensor[0:600],labels_tensor[0:600])\n",
    "    eval_dataset =TensorDataset(features_tensor[600:-1],labels_tensor[600:-1])\n",
    "elif split==3:\n",
    "    train_dataset=TensorDataset(features_tensor[0:600],labels_tensor[0:600])\n",
    "    eval_dataset =TensorDataset(features_tensor[600:-1],labels_tensor[600:-1])\n",
    "elif split==4:\n",
    "    train_dataset=TensorDataset(features_tensor[0:3400],labels_tensor[0:3400])\n",
    "    eval_dataset =TensorDataset(features_tensor[3400:-1],labels_tensor[3400:-1])\n",
    "    # 使用 stratified_split 从 3600 数据中抽取 800 样本，分成训练集和验证集\n",
    "    # train_dataset, eval_dataset = stratified_split(features_tensor, labels_tensor, num_samples_per_class=200, train_ratio=0.75)\n",
    "else:\n",
    "    train_dataset=TensorDataset(features_tensor[0:70],labels_tensor[0:70])\n",
    "    eval_dataset =TensorDataset(features_tensor[70:-1],labels_tensor[70:-1])\n",
    "\n",
    "\n",
    "batch_size=4\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "eval_loader  = DataLoader(eval_dataset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=2,dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        # Define an LSTM with multiple layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True,dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through LSTM\n",
    "        out, (hn, cn) = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  # Use the last time-step's output for classification\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "1Yn_aGGrM923"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nesl/anaconda3/envs/202_proj/lib/python3.9/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "input_size=target_dim+3\n",
    "hidden_size=16\n",
    "output_size=4\n",
    "num_layers = 1     # number of LSTM layers\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, output_size, num_layers=num_layers,dropout=0.4)\n",
    "# model.add(Dense(units))\n",
    "\n",
    "# inputs=features_tensor\n",
    "model = model.to(device)\n",
    "# model = model\n",
    "# inputs = inputs.to(device)\n",
    "# labels = labels_tensor\n",
    "torch.cuda.empty_cache()\n",
    "# outputs = model(inputs)\n",
    "\n",
    "# print(f\"outputs:{outputs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "sEfLDQvmM942",
    "outputId": "66e09942-2516-4d90-ba14-b7fc258a43d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "MHWUEfyINvA0",
    "outputId": "6c2b186f-40d4-4897-f1c7-8ccf15d70e64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:  [1, 2, 1, 2, 1, 2, 3, 0, 2, 3, 3, 2, 3, 0, 2, 1, 2, 3, 0, 2, 3, 2, 0, 0, 0, 3, 0, 3, 3, 1, 3, 0, 2, 2, 2, 0, 0, 3, 1, 3, 1, 2, 0, 3, 0, 1, 2, 0, 0, 3, 0, 3, 3, 0, 2, 1, 2, 2, 2, 3, 0, 3, 0, 2, 3, 3, 2, 3, 0, 0]\n",
      "True Labels:  [1, 0, 0, 3, 1, 2, 1, 3, 2, 0, 1, 1, 3, 0, 0, 0, 3, 1, 1, 3, 3, 3, 1, 0, 0, 1, 3, 0, 1, 0, 2, 0, 3, 2, 2, 1, 3, 1, 1, 3, 1, 2, 0, 3, 0, 0, 3, 3, 0, 2, 2, 3, 3, 2, 2, 3, 2, 0, 2, 2, 0, 3, 2, 2, 0, 1, 2, 2, 2, 3]\n",
      "[1, 2, 3, 6, 7, 9, 10, 11, 14, 15, 16, 17, 18, 19, 21, 22, 25, 26, 27, 28, 29, 30, 32, 35, 36, 37, 45, 46, 47, 49, 50, 53, 55, 57, 59, 62, 64, 65, 67, 68, 69]\n",
      "Test Accuracy: 41.43%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. 损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()  # 分类任务使用交叉熵损失\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001,weight_decay=1e-5)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # Decay learning rate every 10 epochs\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)  # Decrease LR by 5% every epoch\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='min',factor=0.8,patience=5)\n",
    "pretrain=True\n",
    "model_path=\"LSTM_PCA2_256_hidden16_lr0.001_model_epoch_14tran_acc_78.57% val_acc_48.28%.pth\"\n",
    "if pretrain:\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "writer = SummaryWriter('runs/lstm_pca_test')  # This will store logs in 'runs/lstm_training'\n",
    "\n",
    "# 6. 训练循环\n",
    "num_epochs = 1000  # 训练轮次\n",
    "max_accu=0.25\n",
    "model.eval()\n",
    "num=0\n",
    "# Disable gradient computation\n",
    "with torch.no_grad():\n",
    "    test_preds = []  # To store predictions\n",
    "    test_labels = []  # To store true labels\n",
    "    \n",
    "    # Loop through the train_loader (or test_loader if you're evaluating the test set)\n",
    "    for inputs, targets in train_loader:\n",
    "        # Move inputs and targets to the same device as the model\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Ensure the input shape is (batch_size, seq_len, input_size) for LSTM\n",
    "        # Adjust this depending on the actual input shape\n",
    "        # inputs = inputs.view(inputs.size(0), -1, inputs.size(1))  # Assuming inputs have shape (batch_size, input_size)\n",
    "        \n",
    "        # Forward pass through the model\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Get predictions by taking the argmax along the output dimension\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        num+=1\n",
    "        \n",
    "        # Store predictions and true labels as NumPy arrays\n",
    "        test_preds.extend(predicted.cpu().numpy())\n",
    "        test_labels.extend(targets.cpu().numpy())\n",
    "        \n",
    "        # Print predictions and labels for debugging (optional)\n",
    "    print(\"Predictions: \", test_preds)\n",
    "    print(\"True Labels: \", test_labels)\n",
    "\n",
    "    # You may want to calculate accuracy or other metrics after the loop\n",
    "    # For example, calculate accuracy:\n",
    "\n",
    "    accuracy = sum(np.array(test_preds) == np.array(test_labels)) / len(test_labels)\n",
    "    # print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    diff_indices = [i for i in range(len(test_labels)) if test_labels[i] != test_preds[i]]\n",
    "\n",
    "    print(diff_indices)\n",
    "    \n",
    "\n",
    "\n",
    "    # accuracy = accuracy_score(test_labels, test_preds)\n",
    "    print(f\"Test Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gm3nQwrmNvFm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1u9r3Ks1vpGTKk8Y8fmT7pWWTRKB6kU0B",
     "timestamp": 1733522840429
    }
   ]
  },
  "kernelspec": {
   "display_name": "202_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
