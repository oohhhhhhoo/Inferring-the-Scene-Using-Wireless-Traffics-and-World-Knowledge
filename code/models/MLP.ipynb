{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EIo9gb1AeYF6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fOVyQrB4fDbB"
   },
   "outputs": [],
   "source": [
    "def slide_split_get_label_from_filename(filename):\n",
    "    # print(label_mapping.get(filename[9:-5],-1))\n",
    "    return label_mapping.get(filename[9:-5],-1)\n",
    "\n",
    "def split_get_label_from_filename(filename):\n",
    "    return label_mapping.get(filename[8:-5],-1)\n",
    "\n",
    "def get_label_from_filename(filename):\n",
    "    match = re.search(r'_(\\d+_?\\d*)\\.json$', filename)\n",
    "    print(match)\n",
    "    if match:\n",
    "        label_key = match.group(1)\n",
    "        return label_mapping.get(label_key, -1)  # 如果标签不匹配返回 -1\n",
    "\n",
    "# 获取文件夹中的文件及其标签\n",
    "def load_files_and_labels(folder_path,split=False):\n",
    "    file_list = []\n",
    "    labels = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        # print(\"file: \", file)\n",
    "        if file.endswith(\".json\"):\n",
    "            if split == 1 or split==2 or split==3:\n",
    "                label = split_get_label_from_filename(file)\n",
    "            elif split == 4:\n",
    "                # print(f\"split is {split}\")\n",
    "                label = slide_split_get_label_from_filename(file)\n",
    "            else:\n",
    "                label = get_label_from_filename(file)\n",
    "            if label != -1:\n",
    "                file_list.append(os.path.join(folder_path, file))\n",
    "                labels.append(label)\n",
    "            #print(f\"File: {file}, Label: {label}\")\n",
    "\n",
    "    return file_list, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4mfG863jfGeV"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# 1. 提取字符并编码为整数索引\n",
    "def hex_to_sequence(hex_feature):\n",
    "    \"\"\"\n",
    "    将十六进制字符串转换为整数索引序列。\n",
    "    去掉冒号并转换为字符对应的索引。\n",
    "    \"\"\"\n",
    "    hex_chars = hex_feature.replace(\":\", \"\")\n",
    "    char_to_index = {char: idx for idx, char in enumerate(\"0123456789abcdef\")}\n",
    "    return [char_to_index[char] for char in hex_chars]\n",
    "\n",
    "def process_json(file_path):\n",
    "    \"\"\"\n",
    "    处理 JSON 文件，提取时间戳、包长度和 raw data 特征，并对特征进行归一化。\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    try:\n",
    "        # 打开 JSON 文件\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            datas = json.load(file)\n",
    "\n",
    "        # 遍历 JSON 数据，提取时间戳和包长度\n",
    "        initial_timestamp = None\n",
    "        pre_time=None\n",
    "        for data in datas:\n",
    "            try:\n",
    "                timestamp = float(data[\"_source\"][\"layers\"][\"frame\"][\"frame.time_relative\"])  # 时间戳\n",
    "                packet_length = int(data[\"_source\"][\"layers\"][\"frame\"][\"frame.len\"])  # 包长度\n",
    "\n",
    "                # 获取数据部分，若不存在则为0\n",
    "                if 'data' in data[\"_source\"][\"layers\"]:\n",
    "                    rawdata = data[\"_source\"][\"layers\"][\"data\"][\"data.data\"]\n",
    "                else:\n",
    "                    rawdata = '0'\n",
    "\n",
    "                # 将原始数据转换为整数序列\n",
    "                data_feature = hex_to_sequence(rawdata)\n",
    "\n",
    "                # 将数据填充或截断为指定长度 2832\n",
    "                if len(data_feature) < 2832:  # 如果长度小于 2832，填充 0\n",
    "                    data_feature += [0] * (2832 - len(data_feature))\n",
    "                elif len(data_feature) > 2832:  # 如果长度大于 2832，截断\n",
    "                    data_feature = data_feature[:2832]\n",
    "\n",
    "                # 如果数据长度不符合预期，打印出来\n",
    "                if len(data_feature) != 2832:\n",
    "                    print(f\"Data feature length mismatch: {len(data_feature)}\")\n",
    "\n",
    "                # 初始化时间戳\n",
    "                if initial_timestamp is None:\n",
    "                    initial_timestamp = timestamp  # 设置初始时间戳\n",
    "\n",
    "\n",
    "                # 计算相对时间戳\n",
    "\n",
    "                relative_timestamp = timestamp - initial_timestamp\n",
    "                if pre_time==None:\n",
    "                    pre_time=relative_timestamp\n",
    "\n",
    "                time_diff=relative_timestamp-pre_time\n",
    "                pre_time=relative_timestamp\n",
    "\n",
    "                # print(type(relative_timestamp))\n",
    "\n",
    "\n",
    "                timestamp_array = np.array([relative_timestamp], dtype=float)\n",
    "                time_diff_array = np.array([time_diff], dtype=float)\n",
    "\n",
    "                packet_length_array = np.array([packet_length], dtype=float)/1512\n",
    "                data_feature = np.array(data_feature, dtype=float)/15\n",
    "\n",
    "                # 将特征按顺序组合为 [时间戳, 包长度, 数据特征]\n",
    "                feature = np.hstack((timestamp_array,time_diff_array, packet_length_array,data_feature))\n",
    "\n",
    "                # 添加到特征列表\n",
    "                features.append(feature)\n",
    "\n",
    "            except (KeyError, ValueError) as e:\n",
    "                # 跳过有问题的数据包\n",
    "                print(f\"Skipping packet due to error: {e}\")\n",
    "                continue\n",
    "\n",
    "        features_array=np.array(features)\n",
    "        max_timestamp = np.max(features_array[:, 0])  # 获取最大时间戳\n",
    "        max_time_diff = np.max(features_array[:, 1])  # 获取最大时间diff\n",
    "        # print(\"max\")\n",
    "        features_array[:, 0] = [feature[0] / max_timestamp for feature in features_array]  # 时间戳归一化\n",
    "        features_array[:, 1] = [feature[1] / max_time_diff for feature in features_array]  # 时间diff归一化\n",
    "        # print(\"11\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(Exception)\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        features_array = np.zeros((1, 2835))  # 返回空特征以避免程序中断\n",
    "\n",
    "    return features_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-OpQ-4LAfKVs",
    "outputId": "bc027589-c445-4e59-c1cb-bf63e9aad199"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "<class 'list'>\n",
      "(800,)\n",
      "[1 0 2 2 1 3 1 3 1 1 2 1 0 0 3 0 0 0 2 0 2 3 2 3 0 3 0 3 3 0 0 1 2 1 3 1 2\n",
      " 0 2 1 3 3 0 0 2 2 2 3 0 3 3 1 2 1 1 3 0 0 3 3 2 0 1 1 1 0 0 3 0 1 3 2 3 3\n",
      " 1 1 0 2 0 2 3 1 3 0 1 0 0 3 1 3 3 0 1 3 1 1 2 0 3 3 1 1 0 0 1 0 3 3 2 0 2\n",
      " 2 2 3 0 3 2 3 0 3 1 0 0 3 2 0 0 0 0 2 0 3 0 2 3 3 1 0 3 0 1 2 1 2 0 2 0 3\n",
      " 0 1 1 0 2 1 3 0 0 0 3 2 1 1 2 3 3 0 1 3 1 3 3 3 2 1 1 2 3 2 1 1 1 2 2 0 0\n",
      " 0 1 2 2 2 2 0 1 2 1 1 1 2 3 0 1 0 2 3 2 2 0 2 1 1 3 0 3 2 2 1 2 3 0 2 3 0\n",
      " 1 1 0 3 2 1 3 1 2 3 2 1 3 1 0 0 1 3 1 1 2 0 3 3 1 3 3 0 3 3 3 0 2 2 1 1 0\n",
      " 3 2 2 3 3 2 1 1 2 2 1 3 3 3 0 0 3 0 2 3 0 2 0 2 0 2 2 0 0 0 2 2 1 2 0 0 2\n",
      " 3 3 3 1 0 2 2 0 2 1 3 0 0 1 3 0 3 1 0 0 1 0 2 2 2 2 3 2 3 3 2 1 2 1 3 3 1\n",
      " 1 3 1 1 2 1 1 1 3 0 3 0 3 0 0 2 2 0 3 1 1 0 1 1 0 3 0 0 3 3 0 1 0 1 0 3 3\n",
      " 0 0 1 1 0 0 2 0 3 1 3 3 3 0 0 2 2 2 2 1 2 2 0 1 0 2 1 1 3 0 2 2 2 2 3 1 1\n",
      " 1 2 3 1 1 2 1 2 3 0 3 3 3 1 3 2 2 1 1 1 2 2 1 2 2 0 0 0 3 2 3 0 0 2 1 3 3\n",
      " 3 2 3 0 3 3 1 2 2 0 0 0 0 0 3 2 1 0 0 0 2 0 3 2 1 0 1 3 2 2 2 3 3 0 1 3 2\n",
      " 3 0 2 1 0 3 3 3 1 1 3 2 3 3 0 0 3 2 1 0 2 1 1 0 1 3 3 2 1 2 1 1 2 2 2 2 2\n",
      " 1 2 3 2 2 1 1 3 1 2 1 0 0 3 3 0 1 2 0 0 3 2 0 2 2 0 3 1 2 1 1 3 1 3 3 0 1\n",
      " 2 2 1 1 1 2 3 2 0 1 2 2 3 2 2 3 1 0 1 1 2 1 3 1 2 1 2 0 0 3 3 1 0 0 3 3 0\n",
      " 3 3 3 2 2 1 2 2 1 1 2 1 1 1 0 3 2 3 3 2 0 0 2 0 3 3 1 3 1 1 2 1 1 3 2 3 1\n",
      " 1 1 0 3 0 3 0 2 1 2 1 3 2 3 2 1 0 2 3 2 1 0 3 1 3 0 1 2 0 2 2 0 3 0 1 1 2\n",
      " 2 2 0 2 0 1 2 1 1 0 3 2 3 1 0 2 0 2 2 0 0 2 2 1 2 2 0 3 2 0 1 1 2 2 1 3 0\n",
      " 3 1 3 3 0 0 1 3 3 0 0 0 1 2 1 0 3 1 0 0 1 1 1 1 1 0 2 3 3 2 2 3 0 0 1 1 3\n",
      " 1 1 0 2 2 0 1 3 2 3 3 3 2 3 0 0 3 2 1 3 1 0 3 1 1 2 0 0 1 0 2 0 1 3 2 0 0\n",
      " 2 3 3 3 0 2 3 2 3 1 2 3 0 1 1 2 0 3 1 3 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 定义文件夹路径 /content/drive/MyDrive/202_project/202_packet_json\n",
    "\n",
    "split = 2 # 0:100, 1:300, 2:800 3:1000 4:3500\n",
    "# folder_path = \"packet_json_split\"\n",
    "if split==1:\n",
    "    folder_path = \"packet_json_split\"\n",
    "elif split==2:\n",
    "    folder_path = \"202_packet_json_new_800\"\n",
    "elif split==3:\n",
    "    folder_path = \"202_packet_json_new_1000\"\n",
    "elif split==4:\n",
    "    folder_path = \"202_packet_json_new_3500\"\n",
    "else:\n",
    "    folder_path = \"202_packet_json_new\"\n",
    "\n",
    "# 定义标签映射\n",
    "label_mapping = {\n",
    "    \"0\": 0,             # static\n",
    "    \"0_2\": 1,           # slightly_move\n",
    "    \"1\": 2,             # move\n",
    "    \"4\": 3              # intensely_move\n",
    "}\n",
    "\n",
    "# 调用函数获取文件和标签\n",
    "data_paths, labels = load_files_and_labels(folder_path,split)\n",
    "# 用于存储每个样本的特征\n",
    "\n",
    "print(type(labels))\n",
    "labels_array=np.array(labels)#[:50]\n",
    "print(labels_array.shape)\n",
    "print(labels_array)\n",
    "labels_tensor=torch.tensor(labels_array,dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KxgivuFtfNoL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成了 800 个样本特征\n",
      "第一个样本的特征形状: (1486, 2835)\n"
     ]
    }
   ],
   "source": [
    "features_list = []\n",
    "for data_path in data_paths:\n",
    "  features = process_json(data_path)\n",
    "  features_list.append(features)\n",
    "print(f\"生成了 {len(features_list)} 个样本特征\")\n",
    "print(f\"第一个样本的特征形状: {features_list[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# 对齐特征矩阵的形状\n",
    "if split == 1:\n",
    "    fixed_time_steps = 3000\n",
    "elif split == 2:\n",
    "    fixed_time_steps = 1600\n",
    "elif split == 3:\n",
    "    fixed_time_steps = 1200\n",
    "elif split == 4:\n",
    "    fixed_time_steps = 1600\n",
    "else:\n",
    "    fixed_time_steps = 12000\n",
    "\n",
    "# target_dim = 256  # 降到 256 维\n",
    "# pca = PCA(n_components=target_dim)\n",
    "\n",
    "# # 对每个特征矩阵进行降维\n",
    "# reduced_features_list = []\n",
    "# for feature in features_list:\n",
    "#     reduced_feature = pca.fit_transform(feature)  # 对特征维度进行降维\n",
    "#     reduced_features_list.append(reduced_feature)\n",
    "\n",
    "# 截断或补零到固定时间步长\n",
    "\n",
    "aligned_features = []\n",
    "for feature in features_list:\n",
    "    if feature.shape[0] > fixed_time_steps:\n",
    "        truncated = feature[:fixed_time_steps, :]\n",
    "    else:\n",
    "        truncated = np.pad(feature, ((0, fixed_time_steps - feature.shape[0]), (0, 0)), mode='constant')\n",
    "    aligned_features.append(truncated)\n",
    "\n",
    "# Flatten time-step dimension into a single feature vector\n",
    "aligned_features_flat = [feature.flatten() for feature in aligned_features]\n",
    "# Assuming you have already processed your data and have features_tensor and labels_tensor as NumPy arrays\n",
    "features_tensor = np.array(aligned_features_flat)\n",
    "\n",
    "# Ensure labels_tensor is defined\n",
    "# Example:\n",
    "labels_tensor = np.array(labels)  # This should be a NumPy array of the same length as features_tensor\n",
    "\n",
    "# Convert features_tensor and labels_tensor to PyTorch tensors\n",
    "features_tensor = torch.tensor(features_tensor, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(labels_tensor, dtype=torch.long)\n",
    "\n",
    "# Ensure that features_tensor and labels_tensor have the same number of samples\n",
    "assert features_tensor.shape[0] == labels_tensor.shape[0], \"The number of features and labels must match.\"\n",
    "\n",
    "# Data splitting based on 'split' value\n",
    "if split == 1:\n",
    "    train_dataset = TensorDataset(features_tensor[0:210], labels_tensor[0:210])\n",
    "    eval_dataset = TensorDataset(features_tensor[210:], labels_tensor[210:])\n",
    "elif split == 2:\n",
    "    train_dataset = TensorDataset(features_tensor[0:600], labels_tensor[0:600])\n",
    "    eval_dataset = TensorDataset(features_tensor[600:], labels_tensor[600:])\n",
    "elif split == 3:\n",
    "    train_dataset = TensorDataset(features_tensor[0:600], labels_tensor[0:600])\n",
    "    eval_dataset = TensorDataset(features_tensor[600:], labels_tensor[600:])\n",
    "elif split == 4:\n",
    "    train_dataset = TensorDataset(features_tensor[0:3400], labels_tensor[0:3400])\n",
    "    eval_dataset = TensorDataset(features_tensor[3400:], labels_tensor[3400:])\n",
    "else:\n",
    "    train_dataset = TensorDataset(features_tensor[0:70], labels_tensor[0:70])\n",
    "    eval_dataset = TensorDataset(features_tensor[70:], labels_tensor[70:])\n",
    "\n",
    "# Create DataLoader instances for training and evaluation\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "3WCsh2anaDHn"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (inputs, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m---> 47\u001b[0m     features, labels \u001b[38;5;241m=\u001b[39m \u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     48\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "# 定义三层 MLP 模型\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # 第一层全连接\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)  # 第二层全连接\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)  # 输出层\n",
    "        self.relu = nn.ReLU()  # 激活函数\n",
    "        self.dropout = nn.Dropout(0.3)  # Dropout 防止过拟合\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# 初始化模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_size = 2835\n",
    "hidden_size = 64\n",
    "output_size = 4\n",
    "\n",
    "model = SimpleMLP(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "\n",
    "# 损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "# 训练和验证\n",
    "num_epochs = 30\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()  # No change needed\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(features)  # Model outputs logits, no need for softmax here\n",
    "        loss = criterion(outputs, labels)  # Labels are class indices, no one-hot encoding required\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss and accuracy\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_accuracy = correct / total\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for features, labels in eval_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    eval_accuracy = correct / total\n",
    "    print(f\"Validation Loss: {eval_loss:.4f}, Validation Accuracy: {eval_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Classification Report\n",
    "eval_predictions = []\n",
    "eval_true_labels = []\n",
    "with torch.no_grad():\n",
    "    for features, labels in eval_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        outputs = model(features)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        eval_predictions.extend(predicted.cpu().numpy())\n",
    "        eval_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(eval_true_labels, eval_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 100\u001b[0m\n\u001b[1;32m     98\u001b[0m features_list \u001b[38;5;241m=\u001b[39m [process_json(path) \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m data_paths]\n\u001b[1;32m     99\u001b[0m labels_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(labels, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m--> 100\u001b[0m labels_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "# 配置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 定义标签映射\n",
    "label_mapping = {\n",
    "    \"0\": 0,             # static\n",
    "    \"0_2\": 1,           # slightly_move\n",
    "    \"1\": 2,             # move\n",
    "    \"4\": 3              # intensely_move\n",
    "}\n",
    "\n",
    "# 文件路径设置\n",
    "split = 1\n",
    "folder_mapping = {\n",
    "    1: \"packet_json_split\",\n",
    "    2: \"202_packet_json_new_800\",\n",
    "    3: \"202_packet_json_new_1000\",\n",
    "    4: \"202_packet_json_new_3500\"\n",
    "}\n",
    "folder_path = folder_mapping.get(split, \"202_packet_json_new\")\n",
    "\n",
    "# 从文件名获取标签\n",
    "def get_label_from_filename(filename):\n",
    "    match = re.search(r'_(\\d+_?\\d*)\\.json$', filename)\n",
    "    if match:\n",
    "        label_key = match.group(1)\n",
    "        return label_mapping.get(label_key, -1)  # 标签不匹配时返回 -1\n",
    "    return -1\n",
    "\n",
    "# 加载文件及其对应标签\n",
    "def load_files_and_labels(folder_path):\n",
    "    file_list = []\n",
    "    labels = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".json\"):\n",
    "            label = get_label_from_filename(file)\n",
    "            if label != -1:\n",
    "                file_list.append(os.path.join(folder_path, file))\n",
    "                labels.append(label)\n",
    "    return file_list, labels\n",
    "\n",
    "# 将十六进制字符串转换为整数序列\n",
    "def hex_to_sequence(hex_feature):\n",
    "    hex_chars = hex_feature.replace(\":\", \"\")\n",
    "    char_to_index = {char: idx for idx, char in enumerate(\"0123456789abcdef\")}\n",
    "    return [char_to_index[char] for char in hex_chars]\n",
    "\n",
    "# 处理 JSON 文件，提取并归一化特征\n",
    "def process_json(file_path, feature_length=2832):\n",
    "    features = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            datas = json.load(file)\n",
    "\n",
    "        initial_timestamp = None\n",
    "        for data in datas:\n",
    "            try:\n",
    "                timestamp = float(data[\"_source\"][\"layers\"][\"frame\"][\"frame.time_relative\"])\n",
    "                packet_length = int(data[\"_source\"][\"layers\"][\"frame\"][\"frame.len\"])\n",
    "                rawdata = data[\"_source\"][\"layers\"].get(\"data\", {}).get(\"data.data\", '0')\n",
    "\n",
    "                data_feature = hex_to_sequence(rawdata)\n",
    "                data_feature = (data_feature + [0] * feature_length)[:feature_length]  # 填充或截断到指定长度\n",
    "\n",
    "                if initial_timestamp is None:\n",
    "                    initial_timestamp = timestamp\n",
    "                relative_timestamp = timestamp - initial_timestamp\n",
    "\n",
    "                feature = np.hstack((\n",
    "                    relative_timestamp,\n",
    "                    packet_length / 1512,\n",
    "                    np.array(data_feature, dtype=float) / 15\n",
    "                ))\n",
    "                features.append(feature)\n",
    "            except (KeyError, ValueError):\n",
    "                continue\n",
    "\n",
    "        features_array = np.array(features)\n",
    "        features_array[:, 0] /= np.max(features_array[:, 0])  # 时间戳归一化\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        features_array = np.zeros((1, feature_length + 2))\n",
    "    return features_array\n",
    "\n",
    "# 加载数据\n",
    "data_paths, labels = load_files_and_labels(folder_path)\n",
    "features_list = [process_json(path) for path in data_paths]\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Size mismatch between tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 数据集划分\u001b[39;00m\n\u001b[1;32m     14\u001b[0m split_index \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m1\u001b[39m: \u001b[38;5;241m210\u001b[39m, \u001b[38;5;241m2\u001b[39m: \u001b[38;5;241m600\u001b[39m, \u001b[38;5;241m3\u001b[39m: \u001b[38;5;241m600\u001b[39m, \u001b[38;5;241m4\u001b[39m: \u001b[38;5;241m3400\u001b[39m}\u001b[38;5;241m.\u001b[39mget(split, \u001b[38;5;241m70\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTensorDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_tensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43msplit_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_tensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43msplit_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m TensorDataset(features_tensor[split_index:], labels_tensor[split_index:])\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# DataLoader\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/202_proj/lib/python3.9/site-packages/torch/utils/data/dataset.py:192\u001b[0m, in \u001b[0;36mTensorDataset.__init__\u001b[0;34m(self, *tensors)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mtensors: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(tensors[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m tensors), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize mismatch between tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m tensors\n",
      "\u001b[0;31mAssertionError\u001b[0m: Size mismatch between tensors"
     ]
    }
   ],
   "source": [
    "# 对齐特征矩阵形状\n",
    "fixed_time_steps = {1: 3000, 2: 1600, 3: 1200, 4: 1600}.get(split, 12000)\n",
    "aligned_features = []\n",
    "for feature in features_list:\n",
    "    if feature.shape[0] > fixed_time_steps:\n",
    "        aligned_features.append(feature[:fixed_time_steps, :])\n",
    "    else:\n",
    "        aligned_features.append(\n",
    "            np.pad(feature, ((0, fixed_time_steps - feature.shape[0]), (0, 0)), mode='constant')\n",
    "        )\n",
    "features_tensor = torch.tensor(np.stack(aligned_features, axis=0), dtype=torch.float32)\n",
    "\n",
    "# 数据集划分\n",
    "split_index = {1: 210, 2: 600, 3: 600, 4: 3400}.get(split, 70)\n",
    "train_dataset = TensorDataset(features_tensor[:split_index], labels_tensor[:split_index])\n",
    "eval_dataset = TensorDataset(features_tensor[split_index:], labels_tensor[split_index:])\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [16, 4], got [16]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     40\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(features)\n\u001b[0;32m---> 41\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     43\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/202_proj/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/202_proj/lib/python3.9/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/202_proj/lib/python3.9/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [16, 4], got [16]"
     ]
    }
   ],
   "source": [
    "# 定义 MLP 模型\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc3(x)\n",
    "\n",
    "# 初始化模型\n",
    "input_size = 2834  # 特征长度 + 时间戳 + 包长度\n",
    "hidden_size = 64\n",
    "output_size = 4\n",
    "model = SimpleMLP(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# 损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# 训练和验证\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    # 训练阶段\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    for features, labels in train_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_accuracy = correct / total\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {total_loss:.4f}, Train Accuracy: {train_accuracy:.2%}\")\n",
    "\n",
    "    # 验证阶段\n",
    "    model.eval()\n",
    "    eval_loss, correct, total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for features, labels in eval_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            outputs = model(features)\n",
    "            eval_loss += criterion(outputs, labels).item()\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    eval_accuracy = correct / total\n",
    "    print(f\"Validation Loss: {eval_loss:.4f}, Validation Accuracy: {eval_accuracy:.2%}\")\n",
    "\n",
    "# 生成分类报告\n",
    "eval_preds, eval_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for features, labels in eval_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        eval_preds.extend(model(features).argmax(1).cpu().numpy())\n",
    "        eval_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(eval_labels, eval_preds))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "202_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
