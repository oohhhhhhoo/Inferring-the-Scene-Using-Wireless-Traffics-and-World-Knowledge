{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YGw4jevKM2We"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4Uss33YfM9qV"
   },
   "outputs": [],
   "source": [
    "def slide_split_get_label_from_filename(filename):\n",
    "    # print(label_mapping.get(filename[9:-5],-1))\n",
    "    return label_mapping.get(filename[9:-5],-1)\n",
    "\n",
    "def split_get_label_from_filename(filename):\n",
    "    return label_mapping.get(filename[8:-5],-1)\n",
    "\n",
    "def get_label_from_filename(filename):\n",
    "    match = re.search(r'_(\\d+_?\\d*)\\.json$', filename)\n",
    "    print(match)\n",
    "    if match:\n",
    "        label_key = match.group(1)\n",
    "        return label_mapping.get(label_key, -1)  # 如果标签不匹配返回 -1\n",
    "\n",
    "# 获取文件夹中的文件及其标签\n",
    "def load_files_and_labels(folder_path,split=False):\n",
    "    file_list = []\n",
    "    labels = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        # print(\"file: \", file)\n",
    "        if file.endswith(\".json\"):\n",
    "            if split == 1 or split==2 or split==3:\n",
    "                label = split_get_label_from_filename(file)\n",
    "            elif split == 4: \n",
    "                # print(f\"split is {split}\")\n",
    "                label = slide_split_get_label_from_filename(file)\n",
    "            else: \n",
    "                label = get_label_from_filename(file)\n",
    "            if label != -1:\n",
    "                file_list.append(os.path.join(folder_path, file))\n",
    "                labels.append(label)\n",
    "            #print(f\"File: {file}, Label: {label}\")\n",
    "\n",
    "    return file_list, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Do1IExw8M9s0"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# 1. 提取字符并编码为整数索引\n",
    "def hex_to_sequence(hex_feature):\n",
    "    \"\"\"\n",
    "    将十六进制字符串转换为整数索引序列。\n",
    "    去掉冒号并转换为字符对应的索引。\n",
    "    \"\"\"\n",
    "    hex_chars = hex_feature.replace(\":\", \"\")\n",
    "    char_to_index = {char: idx for idx, char in enumerate(\"0123456789abcdef\")}\n",
    "    return [char_to_index[char] for char in hex_chars]\n",
    "\n",
    "def process_json(file_path):\n",
    "    \"\"\"\n",
    "    处理 JSON 文件，提取时间戳、包长度和 raw data 特征，并对特征进行归一化。\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    try:\n",
    "        # 打开 JSON 文件\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            datas = json.load(file)\n",
    "\n",
    "        # 遍历 JSON 数据，提取时间戳和包长度\n",
    "        initial_timestamp = None\n",
    "        for data in datas:\n",
    "            try:\n",
    "                timestamp = float(data[\"_source\"][\"layers\"][\"frame\"][\"frame.time_relative\"])  # 时间戳\n",
    "                packet_length = int(data[\"_source\"][\"layers\"][\"frame\"][\"frame.len\"])  # 包长度\n",
    "\n",
    "                # 获取数据部分，若不存在则为0\n",
    "                if 'data' in data[\"_source\"][\"layers\"]:\n",
    "                    rawdata = data[\"_source\"][\"layers\"][\"data\"][\"data.data\"]\n",
    "                else:\n",
    "                    rawdata = '0'\n",
    "\n",
    "                # 将原始数据转换为整数序列\n",
    "                data_feature = hex_to_sequence(rawdata)\n",
    "\n",
    "                # 将数据填充或截断为指定长度 length    \n",
    "\n",
    "                \n",
    "                if len(data_feature) < 2832:  # 如果长度小于 2832，填充 0\n",
    "                    data_feature += [0] * (2832 - len(data_feature))\n",
    "                elif len(data_feature) > 2832:  # 如果长度大于 2832，截断\n",
    "                    data_feature = data_feature[:2832]\n",
    "\n",
    "                # 如果数据长度不符合预期，打印出来\n",
    "                if len(data_feature) != 2832:\n",
    "                    print(f\"Data feature length mismatch: {len(data_feature)}\")\n",
    "\n",
    "                # 初始化时间戳\n",
    "                if initial_timestamp is None:\n",
    "                    initial_timestamp = timestamp  # 设置初始时间戳\n",
    "\n",
    "                # 计算相对时间戳\n",
    "                relative_timestamp = timestamp - initial_timestamp\n",
    "                # print(type(relative_timestamp))\n",
    "\n",
    "\n",
    "                timestamp_array = np.array([relative_timestamp], dtype=float)\n",
    "                packet_length_array = np.array([packet_length], dtype=float)/1512\n",
    "                data_feature = np.array(data_feature, dtype=float)/15\n",
    "\n",
    "                # 将特征按顺序组合为 [时间戳, 包长度, 数据特征]\n",
    "                feature = np.hstack((timestamp_array, packet_length_array,data_feature))\n",
    "\n",
    "                # 添加到特征列表\n",
    "                features.append(feature)\n",
    "\n",
    "            except (KeyError, ValueError) as e:\n",
    "                # 跳过有问题的数据包\n",
    "                print(f\"Skipping packet due to error: {e}\")\n",
    "                continue\n",
    "\n",
    "        features_array=np.array(features)\n",
    "        max_timestamp = np.max(features_array[:, 0])  # 获取最大时间戳\n",
    "        # print(\"max\")\n",
    "        features_array[:, 0] = [feature[0] / max_timestamp for feature in features_array]  # 时间戳归一化\n",
    "        # print(\"11\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(Exception)\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        features_array = np.zeros((1, 2834))  # 返回空特征以避免程序中断\n",
    "\n",
    "    return features_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 4 # 0:100, 1:300, 2:800 3:1000 4:3500\n",
    "# folder_path = \"packet_json_split\"\n",
    "if split==1:\n",
    "    folder_path = \"packet_json_split\"\n",
    "elif split==2:\n",
    "    folder_path = \"202_packet_json_new_800\"\n",
    "elif split==3:\n",
    "    folder_path = \"202_packet_json_new_1000\"\n",
    "elif split==4:\n",
    "    folder_path = \"202_packet_json_new_3500\"\n",
    "else:\n",
    "    folder_path = \"202_packet_json_new\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "31hdu2h-M9vV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "<class 'list'>\n",
      "(3600,)\n",
      "[2 1 2 ... 3 2 0]\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 定义文件夹路径 /content/drive/MyDrive/202_project/202_packet_json\n",
    "\n",
    "\n",
    "# 定义标签映射\n",
    "label_mapping = {\n",
    "    \"0\": 0,             # static\n",
    "    \"0_2\": 1,           # slightly_move\n",
    "    \"1\": 2,             # move\n",
    "    \"4\": 3              # intensely_move\n",
    "}\n",
    "\n",
    "# 调用函数获取文件和标签\n",
    "data_paths, labels = load_files_and_labels(folder_path,split)\n",
    "# 用于存储每个样本的特征\n",
    "\n",
    "print(type(labels))\n",
    "labels_array=np.array(labels)#[:50]\n",
    "print(labels_array.shape)\n",
    "print(labels_array)\n",
    "labels_tensor=torch.tensor(labels_array,dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56x7MCa8M9xN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成了 3600 个样本特征\n",
      "第一个样本的特征形状: (1658, 2834)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "features_list = []\n",
    "\n",
    "# 遍历每个样本并生成特征\n",
    "# 将特征添加到列表中\n",
    "# 查看生成的 features_list\n",
    "\n",
    "for data_path in data_paths:\n",
    "  # print(data_path)\n",
    "  features = process_json(data_path)\n",
    "  features_list.append(features)\n",
    "  # print(features.shape)\n",
    "print(f\"生成了 {len(features_list)} 个样本特征\")\n",
    "print(f\"第一个样本的特征形状: {features_list[0].shape}\")\n",
    "# print(data_path)\n",
    "# print(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GvcsaqOMM9zJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "if split==1:fixed_time_steps = 3000\n",
    "elif split==2: fixed_time_steps = 1600\n",
    "elif split==3: fixed_time_steps = 1200\n",
    "elif split==4: fixed_time_steps = 1600\n",
    "else: fixed_time_steps = 12000\n",
    "# 截断或补零到固定长度\n",
    "\n",
    "\n",
    "target_dim = 256  # 降到 256 维\n",
    "pca = PCA(n_components=target_dim)\n",
    "\n",
    "# 对每个特征矩阵进行降维\n",
    "reduced_features_list = []\n",
    "for feature in features_list:\n",
    "    reduced_feature = pca.fit_transform(feature)  # 对特征维度进行降维\n",
    "    reduced_features_list.append(reduced_feature)\n",
    "\n",
    "# 截断或补零到固定时间步长\n",
    "\n",
    "aligned_features = []\n",
    "for feature in reduced_features_list:\n",
    "    if feature.shape[0] > fixed_time_steps:\n",
    "        truncated = feature[:fixed_time_steps, :]\n",
    "    else:\n",
    "        truncated = np.pad(feature, ((0, fixed_time_steps - feature.shape[0]), (0, 0)), mode='constant')\n",
    "    aligned_features.append(truncated)\n",
    "\n",
    "\n",
    "features_tensor =  torch.tensor(np.stack(aligned_features, axis=0),dtype=torch.float32)\n",
    "print(f\"截断后的特征张量形状: {features_tensor.shape}\")\n",
    "print(type(features_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "\n",
    "def stratified_split(features_tensor, labels_tensor, num_samples_per_class=200, train_ratio=0.75):\n",
    "    \"\"\"\n",
    "    从数据集中按照标签均匀抽取样本，分为训练集和验证集。\n",
    "    :param features_tensor: 数据特征张量 (N, ...)，N 是样本数\n",
    "    :param labels_tensor: 数据标签张量 (N,)\n",
    "    :param num_samples_per_class: 每个类别抽取的样本数\n",
    "    :param train_ratio: 抽取样本中分配到训练集的比例\n",
    "    :return: train_dataset, eval_dataset\n",
    "    \"\"\"\n",
    "    unique_labels = torch.unique(labels_tensor).tolist()  # 获取所有的标签\n",
    "    train_indices = []\n",
    "    eval_indices = []\n",
    "\n",
    "    for label in unique_labels:\n",
    "        # 找到当前类别对应的索引\n",
    "        label_indices = torch.where(labels_tensor == label)[0]\n",
    "\n",
    "        # 随机打乱\n",
    "        np.random.seed(42)  # 设置随机种子，保证可重复性\n",
    "        shuffled_indices = np.random.permutation(label_indices.cpu().numpy())\n",
    "\n",
    "        # 抽取指定数量的样本\n",
    "        selected_indices = shuffled_indices[:num_samples_per_class]\n",
    "\n",
    "        # 分割为训练集和验证集\n",
    "        num_train_samples = int(train_ratio * num_samples_per_class)\n",
    "        train_indices.extend(selected_indices[:num_train_samples])\n",
    "        eval_indices.extend(selected_indices[num_train_samples:])\n",
    "\n",
    "    # 创建训练集和验证集\n",
    "    train_dataset = Subset(TensorDataset(features_tensor, labels_tensor), train_indices)\n",
    "    eval_dataset = Subset(TensorDataset(features_tensor, labels_tensor), eval_indices)\n",
    "\n",
    "    return train_dataset, eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yJt-YqzXM91D"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m\n\u001b[1;32m     21\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset,batch_size\u001b[38;5;241m=\u001b[39mbatch_size,shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 22\u001b[0m eval_loader  \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLSTMModel\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_size, hidden_size, output_size, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/202_proj/lib/python3.9/site-packages/torch/utils/data/dataloader.py:351\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[0;32m--> 351\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    353\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/202_proj/lib/python3.9/site-packages/torch/utils/data/sampler.py:107\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement))\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue, but got num_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples))\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "if split==1:\n",
    "    train_dataset=TensorDataset(features_tensor[0:210],labels_tensor[0:210])\n",
    "    eval_dataset =TensorDataset(features_tensor[210:-1],labels_tensor[210:-1])\n",
    "elif split==2:\n",
    "    train_dataset=TensorDataset(features_tensor[0:600],labels_tensor[0:600])\n",
    "    eval_dataset =TensorDataset(features_tensor[600:-1],labels_tensor[600:-1])\n",
    "elif split==3:\n",
    "    train_dataset=TensorDataset(features_tensor[0:600],labels_tensor[0:600])\n",
    "    eval_dataset =TensorDataset(features_tensor[600:-1],labels_tensor[600:-1])\n",
    "elif split==4:\n",
    "    train_dataset=TensorDataset(features_tensor[0:3400],labels_tensor[0:3400])\n",
    "    eval_dataset =TensorDataset(features_tensor[3400:-1],labels_tensor[3400:-1])\n",
    "    # 使用 stratified_split 从 3600 数据中抽取 800 样本，分成训练集和验证集\n",
    "    # train_dataset, eval_dataset = stratified_split(features_tensor, labels_tensor, num_samples_per_class=200, train_ratio=0.75)\n",
    "else:\n",
    "    train_dataset=TensorDataset(features_tensor[0:70],labels_tensor[0:70])\n",
    "    eval_dataset =TensorDataset(features_tensor[70:-1],labels_tensor[70:-1])\n",
    "\n",
    "\n",
    "batch_size=4\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "eval_loader  = DataLoader(eval_dataset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=2,dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        # Define an LSTM with multiple layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True,dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through LSTM\n",
    "        out, (hn, cn) = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  # Use the last time-step's output for classification\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Yn_aGGrM923"
   },
   "outputs": [],
   "source": [
    "input_size=target_dim\n",
    "hidden_size=16\n",
    "output_size=4\n",
    "num_layers = 1     # number of LSTM layers\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, output_size, num_layers=num_layers,dropout=0.4)\n",
    "# model.add(Dense(units))\n",
    "\n",
    "# inputs=features_tensor\n",
    "model = model.to(device)\n",
    "# model = model\n",
    "# inputs = inputs.to(device)\n",
    "# labels = labels_tensor\n",
    "torch.cuda.empty_cache()\n",
    "# outputs = model(inputs)\n",
    "\n",
    "# print(f\"outputs:{outputs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sEfLDQvmM942"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHWUEfyINvA0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Learning Rate: 0.001\n",
      "Train Loss: 1.3884, Train Accuracy: 25.83%\n",
      "Eval Loss: 1.3935, Eval Accuracy--23.62%\n",
      "---------------------------------------------------\n",
      "Model saved with Accuracy: Traning--38.33% and Eval--25.13%\n",
      "Epoch [2/1000], Learning Rate: 0.001\n",
      "Train Loss: 1.3170, Train Accuracy: 38.33%\n",
      "Eval Loss: 1.3960, Eval Accuracy--25.13%\n",
      "---------------------------------------------------\n",
      "Model saved with Accuracy: Traning--47.50% and Eval--26.63%\n",
      "Epoch [3/1000], Learning Rate: 0.001\n",
      "Train Loss: 1.2276, Train Accuracy: 47.50%\n",
      "Eval Loss: 1.3984, Eval Accuracy--26.63%\n",
      "---------------------------------------------------\n",
      "Epoch [4/1000], Learning Rate: 0.001\n",
      "Train Loss: 1.0685, Train Accuracy: 55.00%\n",
      "Eval Loss: 1.4810, Eval Accuracy--22.61%\n",
      "---------------------------------------------------\n",
      "Epoch [5/1000], Learning Rate: 0.001\n",
      "Train Loss: 0.9224, Train Accuracy: 59.00%\n",
      "Eval Loss: 1.5416, Eval Accuracy--24.62%\n",
      "---------------------------------------------------\n",
      "Epoch [6/1000], Learning Rate: 0.001\n",
      "Train Loss: 0.8089, Train Accuracy: 62.67%\n",
      "Eval Loss: 1.6626, Eval Accuracy--25.63%\n",
      "---------------------------------------------------\n",
      "Model saved with Accuracy: Traning--62.83% and Eval--27.64%\n",
      "Epoch [7/1000], Learning Rate: 0.001\n",
      "Train Loss: 0.7487, Train Accuracy: 62.83%\n",
      "Eval Loss: 1.7294, Eval Accuracy--27.64%\n",
      "---------------------------------------------------\n",
      "Epoch [8/1000], Learning Rate: 0.001\n",
      "Train Loss: 0.7077, Train Accuracy: 64.50%\n",
      "Eval Loss: 1.8036, Eval Accuracy--24.12%\n",
      "---------------------------------------------------\n",
      "Epoch [9/1000], Learning Rate: 0.001\n",
      "Train Loss: 0.6855, Train Accuracy: 66.00%\n",
      "Eval Loss: 1.8516, Eval Accuracy--27.64%\n",
      "---------------------------------------------------\n",
      "Epoch [10/1000], Learning Rate: 0.001\n",
      "Train Loss: 0.6738, Train Accuracy: 66.00%\n",
      "Eval Loss: 1.9772, Eval Accuracy--23.12%\n",
      "---------------------------------------------------\n",
      "Epoch [11/1000], Learning Rate: 0.001\n",
      "Train Loss: 0.6746, Train Accuracy: 66.00%\n",
      "Eval Loss: 1.9704, Eval Accuracy--26.13%\n",
      "---------------------------------------------------\n",
      "Epoch [12/1000], Learning Rate: 0.001\n",
      "Train Loss: 0.6607, Train Accuracy: 66.00%\n",
      "Eval Loss: 2.0566, Eval Accuracy--24.12%\n",
      "---------------------------------------------------\n",
      "Epoch [13/1000], Learning Rate: 0.001\n",
      "Train Loss: 0.6509, Train Accuracy: 68.00%\n",
      "Eval Loss: 2.1775, Eval Accuracy--25.13%\n",
      "---------------------------------------------------\n",
      "Epoch [14/1000], Learning Rate: 0.001\n",
      "Train Loss: 0.6493, Train Accuracy: 68.17%\n",
      "Eval Loss: 2.1852, Eval Accuracy--26.13%\n",
      "---------------------------------------------------\n",
      "Epoch [15/1000], Learning Rate: 0.001\n",
      "Train Loss: 0.6450, Train Accuracy: 68.00%\n",
      "Eval Loss: 1.9731, Eval Accuracy--26.13%\n",
      "---------------------------------------------------\n",
      "Epoch [16/1000], Learning Rate: 0.001\n",
      "Train Loss: 0.6222, Train Accuracy: 68.33%\n",
      "Eval Loss: 2.0563, Eval Accuracy--24.62%\n",
      "---------------------------------------------------\n",
      "Epoch [17/1000], Learning Rate: 0.001\n",
      "Train Loss: 0.6237, Train Accuracy: 67.17%\n",
      "Eval Loss: 2.1215, Eval Accuracy--26.63%\n",
      "---------------------------------------------------\n",
      "Epoch [18/1000], Learning Rate: 0.001\n",
      "Train Loss: 0.6133, Train Accuracy: 69.00%\n",
      "Eval Loss: 2.0582, Eval Accuracy--24.12%\n",
      "---------------------------------------------------\n",
      "Epoch [19/1000], Learning Rate: 0.001\n",
      "Train Loss: 0.6097, Train Accuracy: 69.83%\n",
      "Eval Loss: 2.1975, Eval Accuracy--25.13%\n",
      "---------------------------------------------------\n",
      "Model saved with Accuracy: Traning--69.00% and Eval--28.64%\n",
      "Epoch [20/1000], Learning Rate: 0.001\n",
      "Train Loss: 0.5926, Train Accuracy: 69.00%\n",
      "Eval Loss: 2.2509, Eval Accuracy--28.64%\n",
      "---------------------------------------------------\n",
      "Epoch [21/1000], Learning Rate: 0.001\n",
      "Train Loss: 0.6413, Train Accuracy: 64.50%\n",
      "Eval Loss: 2.1902, Eval Accuracy--25.13%\n",
      "---------------------------------------------------\n",
      "Epoch [22/1000], Learning Rate: 0.001\n",
      "Train Loss: 0.6310, Train Accuracy: 68.83%\n",
      "Eval Loss: 2.0949, Eval Accuracy--23.62%\n",
      "---------------------------------------------------\n",
      "Epoch [23/1000], Learning Rate: 0.001\n",
      "Train Loss: 0.6294, Train Accuracy: 68.00%\n",
      "Eval Loss: 2.1506, Eval Accuracy--23.62%\n",
      "---------------------------------------------------\n",
      "Epoch [24/1000], Learning Rate: 0.001\n",
      "Train Loss: 0.6023, Train Accuracy: 70.00%\n",
      "Eval Loss: 2.2060, Eval Accuracy--24.62%\n",
      "---------------------------------------------------\n",
      "Epoch [25/1000], Learning Rate: 0.001\n",
      "Train Loss: 0.5866, Train Accuracy: 71.00%\n",
      "Eval Loss: 2.3906, Eval Accuracy--28.64%\n",
      "---------------------------------------------------\n",
      "Epoch [26/1000], Learning Rate: 0.001\n",
      "Train Loss: 0.5663, Train Accuracy: 69.67%\n",
      "Eval Loss: 2.3750, Eval Accuracy--25.13%\n",
      "---------------------------------------------------\n",
      "Epoch [27/1000], Learning Rate: 0.001\n",
      "Train Loss: 0.5757, Train Accuracy: 71.67%\n",
      "Eval Loss: 2.4900, Eval Accuracy--23.62%\n",
      "---------------------------------------------------\n",
      "Epoch [28/1000], Learning Rate: 0.001\n",
      "Train Loss: 0.5720, Train Accuracy: 72.50%\n",
      "Eval Loss: 2.3276, Eval Accuracy--25.13%\n",
      "---------------------------------------------------\n",
      "Epoch [29/1000], Learning Rate: 0.001\n",
      "Train Loss: 0.5943, Train Accuracy: 69.33%\n",
      "Eval Loss: 2.1037, Eval Accuracy--23.12%\n",
      "---------------------------------------------------\n",
      "Epoch [30/1000], Learning Rate: 0.001\n",
      "Train Loss: 0.6669, Train Accuracy: 65.17%\n",
      "Eval Loss: 2.0910, Eval Accuracy--25.63%\n",
      "---------------------------------------------------\n",
      "Epoch [31/1000], Learning Rate: 0.001\n",
      "Train Loss: 0.6303, Train Accuracy: 67.83%\n",
      "Eval Loss: 2.1799, Eval Accuracy--25.13%\n",
      "---------------------------------------------------\n",
      "Epoch [32/1000], Learning Rate: 0.001\n",
      "Train Loss: 0.6266, Train Accuracy: 68.67%\n",
      "Eval Loss: 2.1918, Eval Accuracy--24.62%\n",
      "---------------------------------------------------\n",
      "Epoch [33/1000], Learning Rate: 0.0008\n",
      "Train Loss: 0.6111, Train Accuracy: 69.33%\n",
      "Eval Loss: 2.2818, Eval Accuracy--24.12%\n",
      "---------------------------------------------------\n",
      "Epoch [34/1000], Learning Rate: 0.0008\n",
      "Train Loss: 0.5781, Train Accuracy: 70.50%\n",
      "Eval Loss: 2.3428, Eval Accuracy--21.11%\n",
      "---------------------------------------------------\n",
      "Epoch [35/1000], Learning Rate: 0.0008\n",
      "Train Loss: 0.5513, Train Accuracy: 74.33%\n",
      "Eval Loss: 2.3206, Eval Accuracy--22.11%\n",
      "---------------------------------------------------\n",
      "Epoch [36/1000], Learning Rate: 0.0008\n",
      "Train Loss: 0.5828, Train Accuracy: 70.50%\n",
      "Eval Loss: 2.2715, Eval Accuracy--21.11%\n",
      "---------------------------------------------------\n",
      "Epoch [37/1000], Learning Rate: 0.0008\n",
      "Train Loss: 0.5469, Train Accuracy: 72.33%\n",
      "Eval Loss: 2.3515, Eval Accuracy--21.11%\n",
      "---------------------------------------------------\n",
      "Epoch [38/1000], Learning Rate: 0.0008\n",
      "Train Loss: 0.5383, Train Accuracy: 74.00%\n",
      "Eval Loss: 2.3104, Eval Accuracy--22.11%\n",
      "---------------------------------------------------\n",
      "Epoch [39/1000], Learning Rate: 0.0008\n",
      "Train Loss: 0.5407, Train Accuracy: 74.83%\n",
      "Eval Loss: 2.7894, Eval Accuracy--22.11%\n",
      "---------------------------------------------------\n",
      "Epoch [40/1000], Learning Rate: 0.0008\n",
      "Train Loss: 1.0003, Train Accuracy: 58.33%\n",
      "Eval Loss: 1.5679, Eval Accuracy--26.13%\n",
      "---------------------------------------------------\n",
      "Epoch [41/1000], Learning Rate: 0.0008\n",
      "Train Loss: 1.2843, Train Accuracy: 37.83%\n",
      "Eval Loss: 1.5032, Eval Accuracy--24.62%\n",
      "---------------------------------------------------\n",
      "Epoch [42/1000], Learning Rate: 0.0008\n",
      "Train Loss: 1.1096, Train Accuracy: 51.50%\n",
      "Eval Loss: 1.5047, Eval Accuracy--26.63%\n",
      "---------------------------------------------------\n",
      "Epoch [43/1000], Learning Rate: 0.0008\n",
      "Train Loss: 0.9793, Train Accuracy: 56.67%\n",
      "Eval Loss: 1.5270, Eval Accuracy--22.61%\n",
      "---------------------------------------------------\n",
      "Epoch [44/1000], Learning Rate: 0.0008\n",
      "Train Loss: 0.8977, Train Accuracy: 58.00%\n",
      "Eval Loss: 1.5997, Eval Accuracy--23.62%\n",
      "---------------------------------------------------\n",
      "Epoch [45/1000], Learning Rate: 0.00064\n",
      "Train Loss: 0.8418, Train Accuracy: 61.83%\n",
      "Eval Loss: 1.6012, Eval Accuracy--22.61%\n",
      "---------------------------------------------------\n",
      "Epoch [46/1000], Learning Rate: 0.00064\n",
      "Train Loss: 0.7841, Train Accuracy: 61.83%\n",
      "Eval Loss: 1.6733, Eval Accuracy--23.12%\n",
      "---------------------------------------------------\n",
      "Epoch [47/1000], Learning Rate: 0.00064\n",
      "Train Loss: 0.7661, Train Accuracy: 64.00%\n",
      "Eval Loss: 1.6964, Eval Accuracy--22.61%\n",
      "---------------------------------------------------\n",
      "Epoch [48/1000], Learning Rate: 0.00064\n",
      "Train Loss: 0.7552, Train Accuracy: 62.67%\n",
      "Eval Loss: 1.6965, Eval Accuracy--23.62%\n",
      "---------------------------------------------------\n",
      "Epoch [49/1000], Learning Rate: 0.00064\n",
      "Train Loss: 0.7373, Train Accuracy: 64.67%\n",
      "Eval Loss: 1.6630, Eval Accuracy--20.60%\n",
      "---------------------------------------------------\n",
      "Epoch [50/1000], Learning Rate: 0.00064\n",
      "Train Loss: 0.7148, Train Accuracy: 65.67%\n",
      "Eval Loss: 1.7590, Eval Accuracy--23.62%\n",
      "---------------------------------------------------\n",
      "Epoch [51/1000], Learning Rate: 0.0005120000000000001\n",
      "Train Loss: 0.7204, Train Accuracy: 64.33%\n",
      "Eval Loss: 1.7847, Eval Accuracy--22.61%\n",
      "---------------------------------------------------\n",
      "Epoch [52/1000], Learning Rate: 0.0005120000000000001\n",
      "Train Loss: 0.7108, Train Accuracy: 66.33%\n",
      "Eval Loss: 1.7251, Eval Accuracy--19.10%\n",
      "---------------------------------------------------\n",
      "Epoch [53/1000], Learning Rate: 0.0005120000000000001\n",
      "Train Loss: 0.6923, Train Accuracy: 65.17%\n",
      "Eval Loss: 1.7741, Eval Accuracy--24.12%\n",
      "---------------------------------------------------\n",
      "Epoch [54/1000], Learning Rate: 0.0005120000000000001\n",
      "Train Loss: 0.7009, Train Accuracy: 64.00%\n",
      "Eval Loss: 1.7563, Eval Accuracy--23.62%\n",
      "---------------------------------------------------\n",
      "Epoch [55/1000], Learning Rate: 0.0005120000000000001\n",
      "Train Loss: 0.6979, Train Accuracy: 64.17%\n",
      "Eval Loss: 1.8442, Eval Accuracy--20.10%\n",
      "---------------------------------------------------\n",
      "Epoch [56/1000], Learning Rate: 0.0005120000000000001\n",
      "Train Loss: 0.6834, Train Accuracy: 65.50%\n",
      "Eval Loss: 1.8038, Eval Accuracy--21.11%\n",
      "---------------------------------------------------\n",
      "Epoch [57/1000], Learning Rate: 0.0004096000000000001\n",
      "Train Loss: 0.6767, Train Accuracy: 65.33%\n",
      "Eval Loss: 1.8444, Eval Accuracy--19.60%\n",
      "---------------------------------------------------\n",
      "Epoch [58/1000], Learning Rate: 0.0004096000000000001\n",
      "Train Loss: 0.6726, Train Accuracy: 66.00%\n",
      "Eval Loss: 1.8261, Eval Accuracy--20.10%\n",
      "---------------------------------------------------\n",
      "Epoch [59/1000], Learning Rate: 0.0004096000000000001\n",
      "Train Loss: 0.6538, Train Accuracy: 64.67%\n",
      "Eval Loss: 1.8754, Eval Accuracy--21.61%\n",
      "---------------------------------------------------\n",
      "Epoch [60/1000], Learning Rate: 0.0004096000000000001\n",
      "Train Loss: 0.6553, Train Accuracy: 67.83%\n",
      "Eval Loss: 1.8844, Eval Accuracy--20.10%\n",
      "---------------------------------------------------\n",
      "Epoch [61/1000], Learning Rate: 0.0004096000000000001\n",
      "Train Loss: 0.6771, Train Accuracy: 64.17%\n",
      "Eval Loss: 1.8066, Eval Accuracy--20.10%\n",
      "---------------------------------------------------\n",
      "Epoch [62/1000], Learning Rate: 0.0004096000000000001\n",
      "Train Loss: 0.6631, Train Accuracy: 66.00%\n",
      "Eval Loss: 1.9010, Eval Accuracy--20.10%\n",
      "---------------------------------------------------\n",
      "Epoch [63/1000], Learning Rate: 0.0003276800000000001\n",
      "Train Loss: 0.6664, Train Accuracy: 67.00%\n",
      "Eval Loss: 1.8685, Eval Accuracy--19.60%\n",
      "---------------------------------------------------\n",
      "Epoch [64/1000], Learning Rate: 0.0003276800000000001\n",
      "Train Loss: 0.6482, Train Accuracy: 66.33%\n",
      "Eval Loss: 1.9006, Eval Accuracy--23.62%\n",
      "---------------------------------------------------\n",
      "Epoch [65/1000], Learning Rate: 0.0003276800000000001\n",
      "Train Loss: 0.6456, Train Accuracy: 66.83%\n",
      "Eval Loss: 1.9722, Eval Accuracy--21.11%\n",
      "---------------------------------------------------\n",
      "Epoch [66/1000], Learning Rate: 0.0003276800000000001\n",
      "Train Loss: 0.6302, Train Accuracy: 66.33%\n",
      "Eval Loss: 1.9657, Eval Accuracy--21.11%\n",
      "---------------------------------------------------\n",
      "Epoch [67/1000], Learning Rate: 0.0003276800000000001\n",
      "Train Loss: 0.6446, Train Accuracy: 68.67%\n",
      "Eval Loss: 1.8390, Eval Accuracy--21.61%\n",
      "---------------------------------------------------\n",
      "Epoch [68/1000], Learning Rate: 0.0003276800000000001\n",
      "Train Loss: 0.6470, Train Accuracy: 66.67%\n",
      "Eval Loss: 1.8978, Eval Accuracy--20.60%\n",
      "---------------------------------------------------\n",
      "Epoch [69/1000], Learning Rate: 0.0002621440000000001\n",
      "Train Loss: 0.6226, Train Accuracy: 66.17%\n",
      "Eval Loss: 2.0037, Eval Accuracy--19.10%\n",
      "---------------------------------------------------\n",
      "Epoch [70/1000], Learning Rate: 0.0002621440000000001\n",
      "Train Loss: 0.6299, Train Accuracy: 67.50%\n",
      "Eval Loss: 1.9504, Eval Accuracy--21.11%\n",
      "---------------------------------------------------\n",
      "Epoch [71/1000], Learning Rate: 0.0002621440000000001\n",
      "Train Loss: 0.6199, Train Accuracy: 68.67%\n",
      "Eval Loss: 1.9979, Eval Accuracy--21.11%\n",
      "---------------------------------------------------\n",
      "Epoch [72/1000], Learning Rate: 0.0002621440000000001\n",
      "Train Loss: 0.6114, Train Accuracy: 68.00%\n",
      "Eval Loss: 2.0058, Eval Accuracy--20.10%\n",
      "---------------------------------------------------\n",
      "Epoch [73/1000], Learning Rate: 0.0002621440000000001\n",
      "Train Loss: 0.6141, Train Accuracy: 68.67%\n",
      "Eval Loss: 2.0776, Eval Accuracy--22.61%\n",
      "---------------------------------------------------\n",
      "Epoch [74/1000], Learning Rate: 0.0002621440000000001\n",
      "Train Loss: 0.5989, Train Accuracy: 69.67%\n",
      "Eval Loss: 2.0666, Eval Accuracy--19.10%\n",
      "---------------------------------------------------\n",
      "Epoch [75/1000], Learning Rate: 0.00020971520000000012\n",
      "Train Loss: 0.5904, Train Accuracy: 70.17%\n",
      "Eval Loss: 2.0441, Eval Accuracy--21.11%\n",
      "---------------------------------------------------\n",
      "Epoch [76/1000], Learning Rate: 0.00020971520000000012\n",
      "Train Loss: 0.5982, Train Accuracy: 69.33%\n",
      "Eval Loss: 2.0066, Eval Accuracy--20.10%\n",
      "---------------------------------------------------\n",
      "Epoch [77/1000], Learning Rate: 0.00020971520000000012\n",
      "Train Loss: 0.5750, Train Accuracy: 71.17%\n",
      "Eval Loss: 2.1797, Eval Accuracy--24.12%\n",
      "---------------------------------------------------\n",
      "Epoch [78/1000], Learning Rate: 0.00020971520000000012\n",
      "Train Loss: 0.5730, Train Accuracy: 72.83%\n",
      "Eval Loss: 1.9507, Eval Accuracy--20.60%\n",
      "---------------------------------------------------\n",
      "Epoch [79/1000], Learning Rate: 0.00020971520000000012\n",
      "Train Loss: 0.5638, Train Accuracy: 72.00%\n",
      "Eval Loss: 2.0674, Eval Accuracy--24.12%\n",
      "---------------------------------------------------\n",
      "Epoch [80/1000], Learning Rate: 0.00020971520000000012\n",
      "Train Loss: 0.5630, Train Accuracy: 72.83%\n",
      "Eval Loss: 2.0243, Eval Accuracy--19.60%\n",
      "---------------------------------------------------\n",
      "Epoch [81/1000], Learning Rate: 0.0001677721600000001\n",
      "Train Loss: 0.5278, Train Accuracy: 73.33%\n",
      "Eval Loss: 2.0775, Eval Accuracy--20.10%\n",
      "---------------------------------------------------\n",
      "Epoch [82/1000], Learning Rate: 0.0001677721600000001\n",
      "Train Loss: 0.5374, Train Accuracy: 73.33%\n",
      "Eval Loss: 2.0829, Eval Accuracy--21.11%\n",
      "---------------------------------------------------\n",
      "Epoch [83/1000], Learning Rate: 0.0001677721600000001\n",
      "Train Loss: 0.5085, Train Accuracy: 74.67%\n",
      "Eval Loss: 2.1799, Eval Accuracy--20.10%\n",
      "---------------------------------------------------\n",
      "Epoch [84/1000], Learning Rate: 0.0001677721600000001\n",
      "Train Loss: 0.4982, Train Accuracy: 75.50%\n",
      "Eval Loss: 2.1814, Eval Accuracy--22.11%\n",
      "---------------------------------------------------\n",
      "Epoch [85/1000], Learning Rate: 0.0001677721600000001\n",
      "Train Loss: 0.4918, Train Accuracy: 76.00%\n",
      "Eval Loss: 2.1458, Eval Accuracy--22.11%\n",
      "---------------------------------------------------\n",
      "Epoch [86/1000], Learning Rate: 0.0001677721600000001\n",
      "Train Loss: 0.4767, Train Accuracy: 78.00%\n",
      "Eval Loss: 2.1660, Eval Accuracy--23.62%\n",
      "---------------------------------------------------\n",
      "Epoch [87/1000], Learning Rate: 0.0001677721600000001\n",
      "Train Loss: 0.4649, Train Accuracy: 77.50%\n",
      "Eval Loss: 2.2185, Eval Accuracy--23.12%\n",
      "---------------------------------------------------\n",
      "Epoch [88/1000], Learning Rate: 0.0001677721600000001\n",
      "Train Loss: 0.4593, Train Accuracy: 78.17%\n",
      "Eval Loss: 2.2711, Eval Accuracy--21.61%\n",
      "---------------------------------------------------\n",
      "Epoch [89/1000], Learning Rate: 0.0001677721600000001\n",
      "Train Loss: 0.4618, Train Accuracy: 77.00%\n",
      "Eval Loss: 2.1476, Eval Accuracy--23.12%\n",
      "---------------------------------------------------\n",
      "Epoch [90/1000], Learning Rate: 0.0001677721600000001\n",
      "Train Loss: 0.4847, Train Accuracy: 76.67%\n",
      "Eval Loss: 2.2088, Eval Accuracy--21.11%\n",
      "---------------------------------------------------\n",
      "Epoch [91/1000], Learning Rate: 0.0001677721600000001\n",
      "Train Loss: 0.4854, Train Accuracy: 75.50%\n",
      "Eval Loss: 2.2874, Eval Accuracy--20.60%\n",
      "---------------------------------------------------\n",
      "Epoch [92/1000], Learning Rate: 0.0001677721600000001\n",
      "Train Loss: 0.4487, Train Accuracy: 77.17%\n",
      "Eval Loss: 2.4307, Eval Accuracy--23.12%\n",
      "---------------------------------------------------\n",
      "Epoch [93/1000], Learning Rate: 0.0001677721600000001\n",
      "Train Loss: 0.4925, Train Accuracy: 77.50%\n",
      "Eval Loss: 2.2038, Eval Accuracy--19.60%\n",
      "---------------------------------------------------\n",
      "Epoch [94/1000], Learning Rate: 0.0001677721600000001\n",
      "Train Loss: 0.4794, Train Accuracy: 77.50%\n",
      "Eval Loss: 2.2616, Eval Accuracy--24.12%\n",
      "---------------------------------------------------\n",
      "Epoch [95/1000], Learning Rate: 0.0001677721600000001\n",
      "Train Loss: 0.4349, Train Accuracy: 78.83%\n",
      "Eval Loss: 2.3969, Eval Accuracy--23.12%\n",
      "---------------------------------------------------\n",
      "Epoch [96/1000], Learning Rate: 0.0001677721600000001\n",
      "Train Loss: 0.4314, Train Accuracy: 78.67%\n",
      "Eval Loss: 2.3192, Eval Accuracy--23.12%\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# 更新权重\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# 累计损失\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# 计算正确预测的数量\u001b[39;00m\n\u001b[1;32m     39\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# 获取预测类别\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. 损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()  # 分类任务使用交叉熵损失\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001,weight_decay=1e-5)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # Decay learning rate every 10 epochs\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)  # Decrease LR by 5% every epoch\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='min',factor=0.8,patience=5)\n",
    "pretrain=False\n",
    "model_path=\"model_epoch_21tran_acc_88.57% val_acc_48.28%.pth\"\n",
    "if pretrain:\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "writer = SummaryWriter('runs/lstm_pca')  # This will store logs in 'runs/lstm_training'\n",
    "\n",
    "# 6. 训练循环\n",
    "num_epochs = 1000  # 训练轮次\n",
    "max_accu=0.25\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # 设置模型为训练模式\n",
    "    total_loss = 0  # 累计损失\n",
    "    correct_preds = 0  # 记录正确预测的数量\n",
    "    total_preds = 0  # 总的预测数\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        # 前向传播\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(inputs)  # 获取模型输出\n",
    "        loss = criterion(outputs, targets)  # 计算损失\n",
    "\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()  # 清除梯度\n",
    "        loss.backward()  # 反向传播\n",
    "        optimizer.step()  # 更新权重\n",
    "\n",
    "        # 累计损失\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 计算正确预测的数量\n",
    "        _, predicted = torch.max(outputs, 1)  # 获取预测类别\n",
    "        correct_preds += (predicted == targets).sum().item()  # 统计正确的数量\n",
    "        total_preds += targets.size(0)  # 统计总的预测数\n",
    "\n",
    "    # 计算训练损失和准确率\n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = correct_preds / total_preds\n",
    "\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "\n",
    "      # Log loss to TensorBoard after each epoch\n",
    "    writer.add_scalar('Loss/train', total_loss / len(train_loader), epoch)\n",
    "    writer.add_scalar('Learning Rate', current_lr, epoch)\n",
    "\n",
    "    total_loss=0\n",
    "    correct_preds = 0  # Reset for evaluation\n",
    "    total_preds = 0  # Reset for evaluation\n",
    "\n",
    "    # 7. 评估模型（可选）\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    with torch.no_grad():  # 在评估时不计算梯度\n",
    "        for batch_idx, (inputs, targets) in enumerate(eval_loader):\n",
    "            # 前向传播\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs)  # 获取模型输出\n",
    "            loss = criterion(outputs, targets)  # 计算损失\n",
    "\n",
    "            # 累计损失\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 计算正确预测的数量\n",
    "            _, predicted = torch.max(outputs, 1)  # 获取预测类别\n",
    "            correct_preds += (predicted == targets).sum().item()  # 统计正确的数量\n",
    "            total_preds += targets.size(0)  # 统计总的预测数\n",
    "        # 计算eval损失和准确率\n",
    "    eval_loss = total_loss / len(eval_loader)\n",
    "    eval_accuracy = correct_preds / total_preds\n",
    "    scheduler.step(train_loss)\n",
    "    # scheduler.step()\n",
    "    if eval_accuracy > max_accu:\n",
    "        max_accu = max(eval_accuracy,max_accu)  # Update max accuracy\n",
    "        model_filename = f\"model_epoch_{epoch+1}tran_acc_{train_accuracy*100:.2f}% val_acc_{eval_accuracy*100:.2f}%.pth\"\n",
    "        torch.save(model.state_dict(), model_filename)  # Save model to disko disk\n",
    "        print(f\"Model saved with Accuracy: Traning--{train_accuracy*100:.2f}% and Eval--{eval_accuracy*100:.2f}%\")\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Learning Rate: {current_lr}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy*100:.2f}%\")\n",
    "    print(f\"Eval Loss: {eval_loss:.4f}, Eval Accuracy--{eval_accuracy*100:.2f}%\")\n",
    "    print(\"---------------------------------------------------\")\n",
    "    writer.add_scalars('Loss',{'Train':train_loss,'Eval':eval_loss} , epoch)\n",
    "    writer.add_scalars('Accuracy',{'Train':train_accuracy,'Eval':eval_accuracy} , epoch)\n",
    "\n",
    "writer.close\n",
    "# # Disable gradient computation\n",
    "# with torch.no_grad():\n",
    "#     test_preds = []  # To store predictions\n",
    "#     test_labels = []  # To store true labels\n",
    "\n",
    "#     # Loop through the train_loader (or test_loader if you're evaluating the test set)\n",
    "#     for inputs, targets in train_loader:\n",
    "#         # Move inputs and targets to the same device as the model\n",
    "#         inputs = inputs.to(device)\n",
    "#         targets = targets.to(device)\n",
    "\n",
    "#         # Ensure the input shape is (batch_size, seq_len, input_size) for LSTM\n",
    "#         # Adjust this depending on the actual input shape\n",
    "#         # inputs = inputs.view(inputs.size(0), -1, inputs.size(1))  # Assuming inputs have shape (batch_size, input_size)\n",
    "\n",
    "#         # Forward pass through the model\n",
    "#         outputs = model(inputs)\n",
    "\n",
    "#         # Get predictions by taking the argmax along the output dimension\n",
    "#         _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "#         # Store predictions and true labels as NumPy arrays\n",
    "#         test_preds.extend(predicted.cpu().numpy())\n",
    "#         test_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "#         # Print predictions and labels for debugging (optional)\n",
    "#     print(\"Predictions: \", test_preds)\n",
    "#     print(\"True Labels: \", test_labels)\n",
    "\n",
    "#     # You may want to calculate accuracy or other metrics after the loop\n",
    "#     # For example, calculate accuracy:\n",
    "#     accuracy = sum(np.array(test_preds) == np.array(test_labels)) / len(test_labels)\n",
    "#     # print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "#     # accuracy = accuracy_score(test_labels, test_preds)\n",
    "#     print(f\"Test Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yGJZUDKQNvDd"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Gather predictions and labels for training set\u001b[39;00m\n\u001b[1;32m     35\u001b[0m train_preds, train_labels \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     38\u001b[0m         inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, class_names):\n",
    "    \"\"\"\n",
    "    Calculate and display confusion matrix and metrics.\n",
    "    :param y_true: Ground truth labels.\n",
    "    :param y_pred: Predicted labels.\n",
    "    :param class_names: List of class names.\n",
    "    \"\"\"\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='macro')  # Macro-average for multiclass\n",
    "    recall = recall_score(y_true, y_pred, average='macro')        # Macro-average for multiclass\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision (Macro): {precision:.2f}\")\n",
    "    print(f\"Recall (Macro/Sensitivity): {recall:.2f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# Gather predictions and labels for training set\n",
    "train_preds, train_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_preds.extend(predicted.cpu().numpy())\n",
    "        train_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "# Gather predictions and labels for validation set\n",
    "eval_preds, eval_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in eval_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        eval_preds.extend(predicted.cpu().numpy())\n",
    "        eval_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "# Combine training and validation predictions for overall metrics\n",
    "combined_preds = train_preds + eval_preds\n",
    "combined_labels = train_labels + eval_labels\n",
    "\n",
    "# Define class names for display\n",
    "class_names = [\"Static\", \"Slightly Move\", \"Move\", \"Intensely Move\"]\n",
    "\n",
    "# Calculate and display metrics for training set\n",
    "print(\"Training Set Metrics:\")\n",
    "calculate_metrics(train_labels, train_preds, class_names)\n",
    "\n",
    "# Calculate and display metrics for validation set\n",
    "print(\"\\nValidation Set Metrics:\")\n",
    "calculate_metrics(eval_labels, eval_preds, class_names)\n",
    "\n",
    "# Calculate and display metrics for combined data\n",
    "print(\"\\nOverall Metrics:\")\n",
    "calculate_metrics(combined_labels, combined_preds, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gm3nQwrmNvFm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMWfgEJmvQgnUnof/pzloDK",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "202_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
